from __future__ import annotations
from dataclasses import dataclass
from typing import Dict, Any, List, Optional, Tuple
import json
import re
import hashlib
from collections import Counter
from collections import deque
from pathlib import Path
import time

from .llm_client import LLMClient
from .tools import ToolBox
from .memory import MemoryManagerBase
from .utils import approx_token_count
from .bandit_controller import BanditUnfoldController

def _extract_first_json_object(text: str) -> Optional[str]:
    """Extract the first syntactically balanced JSON object from text.

    This avoids greedy-regex failures when the model outputs multiple brace blocks.
    We scan for balanced braces while respecting quoted strings and escapes.
    """
    s = (text or "").strip()
    start: Optional[int] = None
    depth = 0
    in_str = False
    esc = False

    for i, ch in enumerate(s):
        if start is None:
            if ch == "{":
                start = i
                depth = 1
                in_str = False
                esc = False
            continue

        if esc:
            esc = False
            continue

        if ch == "\\":
            if in_str:
                esc = True
            continue

        if ch == '"':
            in_str = not in_str
            continue

        if in_str:
            continue

        if ch == "{":
            depth += 1
        elif ch == "}":
            depth -= 1
            if depth == 0 and start is not None:
                return s[start:i+1]

    return None

def parse_json_tool_call(text: str) -> Optional[Dict[str, Any]]:
    obj = _extract_first_json_object(text or "")
    if not obj:
        return None
    try:
        parsed = json.loads(obj)
        if isinstance(parsed, dict) and "tool" in parsed and "args" in parsed:
            return parsed
    except Exception:
        return None
    return None

# ---- code version marker (for trace/debug) ----
try:
    from src.version import CODE_VERSION as GOC_CODE_VERSION
except Exception:
    # Fallback so the agent still runs even if version.py was not copied.
    GOC_CODE_VERSION = "unknown"


@dataclass
class ToolLoopConfig:
    max_steps: int = 40
    # Finish gating (prevents premature finish)
    min_steps_before_finish: int = 2
    min_open_pages_before_finish: int = 1
    require_docids_in_finish: bool = True

    # Anti no-finish: nudge/force termination near deadline
    deadline_finish_nudge_steps: int = 3  # nudge when steps remaining <= this
    force_finish_on_deadline: bool = True  # attempt auto-finish at max_steps using opened evidence

    # Break repetitive search loops (common no-finish failure mode)
    repeat_search_consecutive_threshold: int = 6
    auto_open_on_repeat_search: bool = True

    # Break *cyclic* search loops (e.g., alternating between 2-3 queries)
    # This catches non-consecutive repetition patterns that evade the simple streak counter.
    search_cycle_breaker: bool = True
    search_cycle_window: int = 10              # lookback window size
    search_cycle_unique_max: int = 2           # if <= this many unique queries repeat
    search_cycle_min_total: int = 8            # require at least this many recent searches
    search_cycle_require_no_new_candidates: bool = True  # only trigger when searches yield no unopened docids

    # Open-page dedupe + fact header (P1, optional)
    open_page_dedupe: bool = True
    open_page_fact_header: bool = True
    fact_header_max_chars: int = 420

    # Bench-specific policy helpers
    open_given_projects_on_repeat_search: bool = True
    validate_answer_in_given_projects: bool = True
    max_finish_blocks_per_reason: int = 1

    # Candidate-first policy (helps long-horizon tasks with an explicit candidate set)
    # If the question lists a set of candidates (e.g., Project_#### list), prefer opening
    # those candidates' evidence before running global searches.
    candidate_first: bool = True
    candidate_first_min_open_pages: int = 3
    candidate_first_override_global_search: bool = True

    max_json_retries: int = 2

    # Learn-from-failure policy (controller-side constraints)
    # This is designed to reduce repeated unproductive actions (e.g., same search query loops,
    # duplicate open_page calls) without relying on natural-language negation.
    enable_failure_policy: bool = True
    unproductive_search_threshold: int = 2     # block a query after this many no-new-doc searches
    block_search_ttl_steps: int = 10          # how long to cooldown a blocked query (steps)
    duplicate_open_hard_block: bool = True    # if True, block duplicate open_page instead of caching
    max_consecutive_same_tool: int = 6        # if stuck repeating same tool, force a mode switch

    # Adaptive unfolding (GoC only, but safe to call on other memories)
    # The controller dynamically adjusts the unfold budget based on evidence coverage.
    adaptive_unfold: bool = True
    adaptive_unfold_min_step: int = 2
    adaptive_unfold_max_calls: int = 10
    adaptive_unfold_budget_max: int = 900
    adaptive_unfold_buckets: Tuple[int, ...] = (0, 150, 300, 600, 900)

    # Multi-turn (late-binding) task support
    multi_turn_auto_inject: bool = True
    multi_turn_min_step: int = 8
    multi_turn_min_open_pages: int = 3

    # --- Two-stage commit helpers (HotpotQA/FEVER-style; safe on other tasks) ---
    # If the agent commits supporting titles in stage-1 via `return`, optionally
    # enforce that the final `finish` reuses the committed titles to avoid
    # "supporting_titles drift" confounding memory comparisons.
    # Values: "none" | "goc_only" | "all"
    # Enforce committed supporting titles at finish time.
    #   - none: do nothing
    #   - goc_only: only enforce for GoC runs (recommended; stabilizes supporting_titles drift)
    #   - all: enforce for all methods (benchmark-level control)
    enforce_committed_supporting_titles: str = "goc_only"
    committed_supporting_titles_n: int = 2

    # Stage-aware unfolding: on FINAL (often CLOSED-BOOK), proactively unfold the
    # dependency closure around committed anchors / Q1 to counter lost-in-the-middle.
    stage_aware_unfold_on_final: bool = True
    stage_final_unfold_k: int = 6

    # Stage-aware unfolding: on COMMIT (stage-1), proactively unfold the
    # dependency closure around Q1 / earlier evidence to help the model
    # select correct committed titles even if older doc episodes were folded.
    stage_aware_unfold_on_commit: bool = True
    stage_commit_unfold_k: int = 6

    # Option-B: agentic memory controller (fold/unfold)
    enable_agentic_unfold: bool = False
    controller_max_candidates: int = 24
    controller_max_select: int = 6
    controller_output_rationale: bool = False
    enable_agentic_fold: bool = False  # reserved (not enabled by default)

    # --- Bandit controller (GoC-Bandit) ---
    # If enabled, uses a contextual bandit to pick unfold seeds.
    enable_bandit_unfold: bool = False
    bandit_model_path: Optional[str] = None
    bandit_alpha: float = 1.0
    bandit_epsilon: float = 0.05
    bandit_feature_version: str = "v1"

    # --- Optional: LLM-assisted Graph-of-Context construction ---
    # Research toggle to let the acting model emit lightweight annotations
    # that help connect steps in GoC.
    #
    # Modes:
    #   - "none": ignore any "goc" field in the model output (default)
    #   - "hybrid_depends": accept goc.depends_on_steps and ADD extra depends edges
    #   - "tracefirst": also accept goc.step_notes (short) and record them (storage-only)
    goc_annotation_mode: str = "none"

    # --- Annotation prompt gating (minimize tokens) ---
    # Instead of always teaching the model about annotations in the system prompt,
    # we optionally add a *single short hint line* only on gated steps.
    # Comma-separated triggers:
    #   - "doc_switch": the previous open_page switched to a new docid
    #   - "pre_finish": near the end of the episode (remaining <= gate_pre_finish_steps)
    #   - "stage2": only when the current user turn looks like Stage-2 / FINAL
    #   - "stage1": only when the current user turn looks like Stage-1 / COMMIT
    #   - "every_k": additionally gate every K steps (set gate_every_k_steps > 0)
    #   - "commit_turn": any user turn that asks for / COMMIT (two-stage or multi-commit)
    #   - "merge_turn": a user turn that asks for a MERGE decision in multi-commit DAG
    #   - "always": always include the hint line (not recommended)
    goc_annotation_gate: str = "doc_switch,commit_turn,merge_turn,pre_finish"
    goc_annotation_gate_every_k_steps: int = 0
    goc_annotation_gate_pre_finish_steps: int = 2

    # When true, the gated hint uses a MUST-style instruction (higher compliance).
    # When false, the hint is softer ("if relevant"), which may reduce overhead but lowers yield.
    goc_annotation_force: int = 1  # 0=off, 1=force goc key, 2=force minimal dep

    # Compact annotation schema (recommended):
    #   - goc.d : relative step offsets (negative ints), e.g., [-1, -3]
    #   - goc.c : committed-title index (1/2) for HotpotQA-style late binding, e.g., [1]
    # Backward compatible: goc.depends_on_steps remains supported.
    goc_annotation_schema: str = "compact"  # "compact" | "legacy" (parsing supports both)
    # In hybrid_depends, constrain how far back (in LLM tool-call steps) the model can refer.
    goc_declared_dep_max_back: int = 12
    # Cap the number of declared dependencies per step to avoid pathological outputs.
    goc_declared_dep_max_per_step: int = 6
    # In tracefirst, cap note count and length.
    goc_tracefirst_max_notes: int = 3
    goc_tracefirst_note_max_chars: int = 180

    # Debug / logging
    verbose: bool = False                 # print step progress to stdout
    log_dir: Optional[str] = None         # if set, write per-task JSONL trace logs
    log_messages: bool = True             # include sent messages (truncated) in logs
    log_message_chars: int = 6000         # per-message char cap when log_messages=True

    # Prompt context bounding:
    # The memory manager already enforces budget_active (approx tokens). This optional
    # char-level cap is only for safety when debugging very large contexts.
    # 0 = include full ACTIVE_CONTEXT (as produced by the memory manager).
    prompt_context_chars: int = 0

    # Logging truncation (does NOT affect the prompt when prompt_context_chars==0).
    log_context_chars: int = 2500         # tail included in traces/logs
    log_output_chars: int = 4000          # truncate long model outputs in trace

class ToolLoopLLMAgent:
    """Tool-using agent (JSON-only protocol) with robust parsing and optional tracing.

    Key features:
      - JSON-only tool protocol: model must output exactly one JSON object per turn.
      - Auto recovery: if JSON parsing fails, reprompt up to max_json_retries.
      - Accurate tool statistics: total tool calls, search/open_page counts, repeated searches.
      - Optional debug trace logs for LLM input/output per step.
    """

    def __init__(
        self,
        llm: LLMClient,
        tools: ToolBox,
        mem: MemoryManagerBase,
        cfg: Optional[ToolLoopConfig] = None,
        controller_llm: Optional[LLMClient] = None,
        bandit_controller: Optional[BanditUnfoldController] = None,
    ):
        self.llm = llm
        self.controller_llm = controller_llm or llm
        self.bandit_controller = bandit_controller
        self.tools = tools
        self.mem = mem
        self.cfg = cfg or ToolLoopConfig()

        self.usage_accum: Dict[str, int] = {"total_tokens": 0, "input_tokens": 0, "output_tokens": 0}
        self.counters: Counter = Counter()
        self.search_query_counts: Counter = Counter()

        # Evidence / anti-loop helpers
        self.evidence_docids: List[str] = []  # ordered unique docids opened via open_page
        # Lossless store of opened documents (full content). This is NOT injected verbatim into the prompt.
        self.opened_cache: Dict[str, str] = {}  # docid -> full page content
        # Cache per-view opens so we can dedupe exact repeats while still allowing tail/find/offset views.
        # key -> cached view snippet (for tracing / cached observations)
        self.opened_view_cache: Dict[str, str] = {}
        self.given_projects: List[str] = []
        self._given_projects_open_idx: int = 0
        self._finish_block_counts: Counter = Counter()
        self._deadline_nudged: bool = False
        self._last_search_query: Optional[str] = None
        self._last_search_results: List[str] = []
        self._last_search_repeat_streak: int = 0
        self._last_search_open_idx: int = 0
        self._last_block_key: Optional[str] = None
        self._last_block_count: int = 0

        # --- GoC annotation prompt gating state (to reduce tokens) ---
        self._goc_doc_switch_pending: bool = False
        self._goc_last_open_docid: Optional[str] = None
        self._goc_stage1_hint_sent: bool = False
        self._goc_stage2_hint_sent: bool = False
        self._goc_commit_anchor_nid: Optional[str] = None
        self._goc_commit_title_nids: Dict[int, str] = {}

        # Failure-policy state ("constraints" that are enforced deterministically)
        self._blocked_queries: Dict[str, int] = {}          # normalized query -> expires_step
        self._unproductive_queries: Counter = Counter()     # normalized query -> count(no-new-doc)
        self._constraints_written: set = set()              # keys to avoid spamming duplicates
        self._failures_written: set = set()                 # keys to avoid spamming failure nodes
        self._last_progress_step: int = 0                   # last step index with measurable progress
        self._last_exec_tool: Optional[str] = None          # executed tool name
        self._exec_tool_streak: int = 0                     # consecutive same executed tool

        # Cyclic search-loop detection (alternating queries)
        self._recent_search_norms = deque(maxlen=max(4, int(self.cfg.search_cycle_window)))
        self._recent_search_newcand = deque(maxlen=max(4, int(self.cfg.search_cycle_window)))
        self._last_loop_escape_step: Optional[int] = None

        # Adaptive unfolding bookkeeping
        self._adaptive_unfold_calls: int = 0
        self._last_finish_block_reason: Optional[str] = None

        # Two-stage commit helpers
        self._q1_text: str = ""                       # extracted Q1 for multi-turn benches (if present)
        self._committed_supporting_titles: Optional[List[str]] = None
        # Multi-commit: keep per-commit committed artifacts.
        self._committed_supporting_titles_by_commit: Dict[int, List[str]] = {}
        self._committed_answers_by_commit: Dict[int, str] = {}
        self._goc_commit_anchor_nids_by_commit: Dict[int, str] = {}
        self._goc_commit_title_nids_by_commit: Dict[int, Dict[int, str]] = {}
        self._stage_final_unfold_done: bool = False
        # For multi-commit tasks, run stage-aware commit unfold per commit index.
        self._stage_commit_unfold_done: bool = False
        self._stage_commit_unfold_done_for: set = set()

        # Optional: LLM-declared GoC connectivity (per-task reset in run()).
        self._llm_step_to_tool_nid: Dict[int, str] = {}
        self._tracefirst_note_seq: int = 0

        self._trace_fp = None
        self._trace_path: Optional[Path] = None

    def _open_trace(self, run_tag: str, method: str, task_id: str):
        if not self.cfg.log_dir:
            return
        log_dir = Path(self.cfg.log_dir)
        log_dir.mkdir(parents=True, exist_ok=True)
        safe = re.sub(r"[^a-zA-Z0-9_\-]+", "_", f"{run_tag}_{method}_{task_id}")
        self._trace_path = log_dir / f"trace_{safe}.jsonl"
        self._trace_fp = open(self._trace_path, "w", encoding="utf-8")
        # write one meta line for version/debug
        self._trace({
            "type": "run_meta",
            "task_id": task_id,
            "method": method,
            "run_tag": run_tag,
            "code_version": GOC_CODE_VERSION,
        })

    def _close_trace(self):
        if self._trace_fp:
            try:
                self._trace_fp.close()
            finally:
                self._trace_fp = None

    def _trace(self, obj: Dict[str, Any]):
        if not self._trace_fp:
            return
        self._trace_fp.write(json.dumps(obj, ensure_ascii=False) + "\n")
        self._trace_fp.flush()

    # --- GoC annotation helpers (research) ---
    def _maybe_apply_goc_annotations(
        self,
        *,
        call: Optional[Dict[str, Any]],
        cur_step: int,
        tool_nid: str,
        task_id: str,
        method: str,
        run_tag: str,
    ) -> None:
        """Optionally ingest model-declared GoC annotations.

        The mainline GoC implementation infers edges post-hoc. This helper enables
        controlled experiments where the acting model can declare a small set of
        dependencies (hybrid) and/or short step notes (trace-first).

        cur_step is 1-based tool-call index (internal). Compact schema uses relative offsets,
        so the index does not need to be exposed to the model.
        """
        mode = str(getattr(self.cfg, "goc_annotation_mode", "none") or "none").lower().strip()
        if mode in ("none", "off", "false", "0"):
            return
        if not isinstance(call, dict):
            return
        goc = call.get("goc")
        if not isinstance(goc, dict):
            return

        # ---- declared dependency edges (hybrid) ----
        # Supports both legacy absolute indices (depends_on_steps) and compact relative offsets (d).
        depends_steps = goc.get("depends_on_steps")
        if depends_steps is None:
            depends_steps = goc.get("depends_on")

        dep_abs: List[int] = []
        if isinstance(depends_steps, (list, tuple)):
            for x in depends_steps:
                try:
                    dep_abs.append(int(x))
                except Exception:
                    continue
        elif isinstance(depends_steps, (int, str)):
            try:
                dep_abs = [int(depends_steps)]
            except Exception:
                dep_abs = []

        rel = goc.get("d")
        if rel is None:
            rel = goc.get("depends_rel")
        dep_rel: List[int] = []
        if isinstance(rel, (list, tuple)):
            for x in rel:
                try:
                    dep_rel.append(int(x))
                except Exception:
                    continue
        elif isinstance(rel, (int, str)):
            try:
                dep_rel = [int(rel)]
            except Exception:
                dep_rel = []

        max_back = int(getattr(self.cfg, "goc_declared_dep_max_back", 12) or 12)
        max_per = int(getattr(self.cfg, "goc_declared_dep_max_per_step", 6) or 6)

        # Convert relative offsets (negative) to absolute indices.
        dep_from_rel: List[int] = []
        for off in dep_rel:
            if not isinstance(off, int):
                continue
            if off >= 0:
                continue
            abs_i = int(cur_step) + int(off)
            if abs_i >= 1 and abs_i < cur_step and (cur_step - abs_i) <= max_back:
                dep_from_rel.append(abs_i)

        # Filter + merge
        dep_abs = [d for d in dep_abs if d >= 1 and d < cur_step and (cur_step - d) <= max_back]
        merged = dep_abs + dep_from_rel
        # de-dup preserving order
        seen = set()
        dep_list: List[int] = []
        for d in merged:
            if d not in seen:
                seen.add(d)
                dep_list.append(d)
        dep_list = dep_list[:max_per]

        added: List[Tuple[int, str]] = []
        skipped: List[Dict[str, Any]] = []
        if mode in ("hybrid_depends", "tracefirst") and dep_list:
            for d in dep_list:
                parent = self._llm_step_to_tool_nid.get(d)
                if not parent:
                    # Fallback for potential off-by-one step indexing mismatches across runners.
                    parent = self._llm_step_to_tool_nid.get(d - 1) or self._llm_step_to_tool_nid.get(d + 1)
                if not parent:
                    skipped.append({"step": int(d), "reason": "parent_missing"})
                    continue
                try:
                    ok = bool(self.mem.add_edge("depends_llm", tool_nid, parent))
                except Exception:
                    ok = False
                if ok:
                    added.append((d, parent))
                else:
                    skipped.append({"step": int(d), "reason": "edge_add_failed", "parent": parent})

        if added:
            self._trace({
                "type": "goc_declared_depends",
                "task_id": task_id,
                "method": method,
                "run_tag": run_tag,
                "step": cur_step,
                "tool_nid": tool_nid,
                "edge_type": "depends_llm",
                "declared": [{"step": d, "nid": nid} for d, nid in added],
            })
        elif dep_list:
            # Debug visibility: model declared deps but none were added.
            # Provide structured reasons so we can fix stepâ†”node mapping quickly.
            self._trace({
                "type": "goc_declared_depends_skipped",
                "task_id": task_id,
                "method": method,
                "run_tag": run_tag,
                "step": cur_step,
                "tool_nid": tool_nid,
                "edge_type": "depends_llm",
                "declared_steps": dep_list,
                "known_steps": sorted(list(self._llm_step_to_tool_nid.keys()))[:80],
                "skip_details": skipped[:40],
            })

        # ---- commit-anchor shorthand (compact) ----
        # goc.c can refer to committed supporting title index (1/2) without exposing global step indices.
        c_raw = goc.get("c")
        if c_raw is None:
            c_raw = goc.get("commit")
        c_list: List[int] = []
        if isinstance(c_raw, (list, tuple)):
            for x in c_raw:
                try:
                    c_list.append(int(x))
                except Exception:
                    continue
        elif isinstance(c_raw, (int, str)):
            try:
                c_list = [int(c_raw)]
            except Exception:
                c_list = []

        c_list = [c for c in c_list if c >= 1 and c <= 8]
        # de-dup
        seen_c = set()
        c_list2: List[int] = []
        for c in c_list:
            if c not in seen_c:
                seen_c.add(c)
                c_list2.append(c)
        c_list = c_list2[: min(2, max_per)]

        commit_added: List[Tuple[int, str]] = []
        if c_list and mode in ("hybrid_depends", "tracefirst"):
            for c in c_list:
                target = None
                # In two-stage tasks, c usually means committed-title index (1/2) for the *last* commit.
                # In multi-commit DAG tasks, c can mean the commit index itself (1..N).
                anchor_map = getattr(self, "_goc_commit_anchor_nids_by_commit", None)
                title_map_last = getattr(self, "_goc_commit_title_nids", None)
                multi_commit = isinstance(anchor_map, dict) and len(anchor_map) > 1

                if (not multi_commit) and isinstance(title_map_last, dict) and c in title_map_last:
                    target = title_map_last.get(c)
                elif multi_commit and isinstance(anchor_map, dict) and c in anchor_map:
                    target = anchor_map.get(c)
                elif isinstance(title_map_last, dict) and c in title_map_last:
                    target = title_map_last.get(c)
                elif isinstance(anchor_map, dict) and c in anchor_map:
                    target = anchor_map.get(c)

                if not target:
                    target = getattr(self, "_goc_commit_anchor_nid", None)
                if not target:
                    continue
                try:
                    ok = bool(self.mem.add_edge("depends_llm", tool_nid, target))
                except Exception:
                    ok = False
                if ok:
                    commit_added.append((c, target))

        if commit_added:
            self._trace({
                "type": "goc_declared_commit",
                "task_id": task_id,
                "method": method,
                "run_tag": run_tag,
                "step": cur_step,
                "tool_nid": tool_nid,
                "edge_type": "depends_llm",
                "declared": [{"c": c, "nid": nid} for c, nid in commit_added],
            })

        # ---- optional step notes (trace-first) ----
        if mode == "tracefirst":
            notes = goc.get("step_notes")
            if isinstance(notes, str):
                notes_list = [notes]
            elif isinstance(notes, (list, tuple)):
                notes_list = [str(n) for n in notes if n is not None]
            else:
                notes_list = []

            nmax = int(getattr(self.cfg, "goc_tracefirst_max_notes", 3) or 3)
            cmax = int(getattr(self.cfg, "goc_tracefirst_note_max_chars", 180) or 180)
            notes_list = [n.strip() for n in notes_list if str(n).strip()][:nmax]
            notes_list = [n[:cmax] for n in notes_list]
            if notes_list:
                self._trace({
                    "type": "goc_trace_notes",
                    "task_id": task_id,
                    "method": method,
                    "run_tag": run_tag,
                    "step": cur_step,
                    "tool_nid": tool_nid,
                    "notes": notes_list,
                })

    def _record_tool_step(
        self,
        *,
        step0: int,
        call: Optional[Dict[str, Any]],
        tool_name: str,
        args: Dict[str, Any],
        observation: str,
        docids: Optional[List[str]] = None,
        storage_text: Optional[str] = None,
        task_id: str,
        method: str,
        run_tag: str,
    ) -> str:
        """Record a tool call to memory and ingest optional GoC annotations."""
        nid = self.mem.record_tool(tool_name, args, observation, docids=docids, storage_text=storage_text)
        # Defensive: in some user forks, record_tool() mistakenly returns None even though
        # it appended a node into ACTIVE. Recover the last ACTIVE node id when possible.
        if not nid:
            try:
                last = getattr(self.mem, "active", None)
                if isinstance(last, list) and last:
                    cand = last[-1]
                    if isinstance(cand, str) and cand:
                        nid = cand
            except Exception:
                pass
        # 1-based tool-call index (internal). Legacy goc.depends_on_steps can reference this,
        # but we no longer expose it by default (compact schema prefers relative offsets).
        cur_step = int(step0) + 1
        if nid:
            self._llm_step_to_tool_nid[cur_step] = nid
        # Trace raw model-declared GoC annotations (if any) for easier debugging/grepping.
        goc_raw = call.get("goc") if isinstance(call, dict) else None
        if isinstance(goc_raw, dict) and (goc_raw or bool(getattr(self.cfg, "goc_annotation_force", False))):
            self._trace({
                "type": "goc_annotation_raw",
                "task_id": task_id,
                "method": method,
                "run_tag": run_tag,
                "step": cur_step,
                "tool_nid": nid,
                "goc": goc_raw,
            })
        # If nid is still missing, we cannot attach graph edges. Emit a debug event and move on.
        if not nid:
            if isinstance(goc_raw, dict) and goc_raw:
                self._trace({
                    "type": "goc_declared_depends_skipped",
                    "task_id": task_id,
                    "method": method,
                    "run_tag": run_tag,
                    "step": cur_step,
                    "tool_nid": None,
                    "edge_type": "depends_llm",
                    "declared_steps": [],
                    "known_steps": sorted(list(self._llm_step_to_tool_nid.keys()))[:80],
                    "reason": "tool_nid_missing",
                })
            return ""

        self._maybe_apply_goc_annotations(call=call, cur_step=cur_step, tool_nid=nid, task_id=task_id, method=method, run_tag=run_tag)
        return nid

    def _drain_mem_events(self, *, task_id: str, method: str, run_tag: str, step: int):
        """Drain structured events from the memory manager and write to the trace.

        Memory managers may emit events (e.g., GoC fold decisions). This keeps
        trace files self-contained for paper figures and ablations.
        """
        try:
            evs = self.mem.drain_events()
        except Exception:
            return
        if not evs:
            return
        for ev in evs:
            if not isinstance(ev, dict):
                continue
            obj = dict(ev)
            # Attach run metadata for easier post-hoc analysis.
            obj.setdefault("task_id", task_id)
            obj.setdefault("method", method)
            obj.setdefault("run_tag", run_tag)
            obj.setdefault("step", int(step))
            self._trace(obj)

    def _accum_usage(self, usage: Optional[Dict[str, Any]]):
        if not usage:
            return
        total = usage.get("total_tokens")
        if total is None:
            pt = usage.get("prompt_tokens") or 0
            ct = usage.get("completion_tokens") or 0
            total = pt + ct
        self.usage_accum["total_tokens"] += int(total or 0)
        self.usage_accum["input_tokens"] += int(usage.get("input_tokens") or usage.get("prompt_tokens") or 0)
        self.usage_accum["output_tokens"] += int(usage.get("output_tokens") or usage.get("completion_tokens") or 0)

    def _normalize_query(self, q: str) -> str:
        q = (q or "").strip().lower()
        q = re.sub(r"\s+", " ", q)
        q = re.sub(r"[\,\.;:!?\(\)\[\]\{\}\"\']", "", q)
        return q

    def _record_search_query(self, query: str):
        norm = self._normalize_query(query)
        self.search_query_counts[norm] += 1
        if self.search_query_counts[norm] >= 2:
            self.counters["repeated_search_count"] += 1

    # =========================
    # Learn-from-failure policy
    # =========================
    def _sig(self, prefix: str, payload: str) -> str:
        """Stable short signature used as a pseudo-docid for GoC doc_ref linking."""
        s = (payload or "").encode("utf-8", errors="ignore")
        h = hashlib.md5(s).hexdigest()[:12]
        prefix = re.sub(r"[^A-Z0-9_]+", "_", (prefix or "SIG").upper())
        return f"SIG_{prefix}_{h}"

    def _sig_search_docid(self, query: str) -> str:
        return self._sig("SEARCH", self._normalize_query(query))

    def _sig_open_docid(self, docid: str) -> str:
        # docid is already stable; include it in the signature for trace readability.
        return f"SIG_OPEN_{docid}"

    def _policy_record_constraint_once(self, key: str, text: str, *, docids: Optional[List[str]] = None):
        """Record a durable constraint note once.

        We store constraints as SUMMARY nodes so GoC retains them as anchors during pruning.
        """
        if not self.cfg.enable_failure_policy:
            return
        if key in self._constraints_written:
            return
        self._constraints_written.add(key)
        try:
            self.mem.record_summary(text, docids=docids)
            self.counters["constraints_written"] += 1
        except Exception:
            # Never let constraint recording crash the run
            self.counters["constraints_write_errors"] += 1

    def _policy_record_failure_once(self, key: str, text: str, *, docids: Optional[List[str]] = None):
        """Record a failure trace node once.

        Failure traces are stored as SUMMARY nodes with [FAIL] prefix so GoC can
        keep them as anchors in ACTIVE_CONTEXT and also connect them via doc_ref
        to related tool steps (using signature docids).
        """
        if not self.cfg.enable_failure_policy:
            return
        if key in self._failures_written:
            return
        self._failures_written.add(key)
        try:
            self.mem.record_summary(text, docids=docids)
            self.counters["failure_nodes_written"] += 1
        except Exception:
            self.counters["failure_nodes_write_errors"] += 1

    def _policy_candidate_docid(self) -> Optional[str]:
        """Pick a reasonable next docid to open to escape loops."""
        # Prefer unopened given projects (common in SyntheticBrowseComp)
        if self.cfg.open_given_projects_on_repeat_search and self.given_projects:
            did = self._next_unopened_given_docid()
            if did:
                return did
        # Else, use last search results as a pool
        for did in (self._last_search_results or []):
            if did and did not in self.opened_cache:
                return did
        return None

    def _is_globalish_search(self, query: str) -> bool:
        """Heuristic: a search that likely ranges over *all* projects rather than a given candidate set."""
        ql = (query or "").lower()
        if "project_" in ql:
            return False
        # If the question provides a candidate set, searches referencing attributes but not specific IDs
        # are often global. Keep this heuristic lightweight (avoid overfitting to a single dataset).
        attr_terms = ("code_name", "key_number", "start_year", "headquarters", "related_projects")
        if any(t in ql for t in attr_terms) and ("project" in ql or "projects" in ql):
            return True
        if "list" in ql and "projects" in ql:
            return True
        return False

    def _should_break_search_cycle(self, qnorm_next: str) -> bool:
        """Detect cyclic search loops (e.g., alternating between 2-3 queries).

        We trigger only when (a) recent queries are low-entropy (few unique) AND
        (b) they have not been yielding new *unopened* docids.
        """
        if not self.cfg.search_cycle_breaker:
            return False

        win = int(self.cfg.search_cycle_window)
        min_total = int(self.cfg.search_cycle_min_total)
        uniq_max = int(self.cfg.search_cycle_unique_max)
        if win <= 0 or min_total <= 0:
            return False

        norms = list(self._recent_search_norms)
        newc = list(self._recent_search_newcand)
        # Project the next query into the window; assume it won't magically create new candidates.
        if qnorm_next:
            norms = (norms + [qnorm_next])[-win:]
            if self.cfg.search_cycle_require_no_new_candidates:
                newc = (newc + [0])[-win:]
        if len(norms) < min_total:
            return False
        if len(set(norms)) > uniq_max:
            return False
        if self.cfg.search_cycle_require_no_new_candidates:
            if sum(1 for x in newc if x) > 0:
                return False
        return True

    def _infer_required_fields(self, question: str) -> List[str]:
        """Best-effort extraction of which fields are needed from the question."""
        ql = (question or "").lower()
        fields: List[str] = []
        # Heuristics tailored to the benchmark schema but phrased as generic field-keyword matches.
        if "headquarters" in ql or "hq" in ql:
            fields.append("headquarters")
        if "start_year" in ql or "start year" in ql or "earliest" in ql or "oldest" in ql:
            fields.append("start_year")
        if "code_name" in ql or "code name" in ql or "codename" in ql:
            fields.append("code_name")
        if "key_number" in ql or "key number" in ql:
            fields.append("key_number")
        if "related_projects" in ql or "related projects" in ql:
            fields.append("related_projects")
        # If no explicit hint, default to a small core set.
        if not fields:
            fields = ["start_year", "headquarters"]
        # Stable order, dedupe
        out = []
        for f in fields:
            if f not in out:
                out.append(f)
        return out

    def _parse_facts_from_active(self) -> Dict[str, Dict[str, str]]:
        """Extract (project -> {field -> value}) from ACTIVE_CONTEXT."""
        txt = self.mem.get_active_text() or ""
        proj_map: Dict[str, Dict[str, str]] = {}

        # FACTS header format: "FACTS: project=Project_0001 | start_year=1999 | headquarters=City_01 ..."
        for ln in txt.splitlines():
            if "FACTS:" not in ln:
                continue
            # keep after FACTS:
            seg = ln.split("FACTS:", 1)[-1]
            parts = [p.strip() for p in seg.split("|") if p.strip()]
            kv = {}
            for p in parts:
                if "=" in p:
                    k, v = p.split("=", 1)
                    kv[k.strip().lower()] = v.strip()
            proj = kv.get("project")
            if proj and re.match(r"^Project_\d{4}$", proj):
                d = proj_map.setdefault(proj, {})
                for k, v in kv.items():
                    if k in ("start_year", "headquarters", "code_name", "key_number", "related_projects") and v:
                        d.setdefault(k, v)

        # Also accept raw profile lines in open_page snippets
        cur_proj = None
        for ln in txt.splitlines():
            m = re.search(r"\bProject_\d{4}\b", ln)
            if m:
                cur_proj = m.group(0)
                proj_map.setdefault(cur_proj, {})
            if not cur_proj:
                continue
            for key in ("start_year", "headquarters", "code_name", "key_number", "related_projects"):
                m2 = re.search(rf"(?i)\b{key}\b\s*[:=]\s*([^\n]+)", ln)
                if m2:
                    v = m2.group(1).strip()
                    if v:
                        proj_map[cur_proj].setdefault(key, v)

        return proj_map

    def _coverage_and_missing(self, required_fields: List[str]) -> Tuple[float, List[Tuple[str, str]]]:
        """Return (coverage_ratio, missing_pairs[(project, field)])."""
        if not self.given_projects:
            return 1.0, []
        facts = self._parse_facts_from_active()
        denom = max(1, len(self.given_projects) * max(1, len(required_fields)))
        have = 0
        missing: List[Tuple[str, str]] = []
        for p in self.given_projects:
            pf = facts.get(p, {})
            for f in required_fields:
                if pf.get(f):
                    have += 1
                else:
                    missing.append((p, f))
        return have / float(denom), missing


    def _controller_select_unfold_seed_ids(
        self,
        *,
        query: str,
        candidates: List[Dict[str, Any]],
        k: int,
        budget_unfold: int,
        step: int,
        task_id: str,
        method: str,
        run_tag: str,
        reason: str,
    ) -> Tuple[List[str], bool, str]:
        """Ask a small controller LLM to pick which seed ids to unfold.

        Returns (seed_ids, used_fallback, raw_text).
        """
        if not candidates:
            return [], True, ""

        max_sel = max(1, min(int(self.cfg.controller_max_select or 6), int(k or 1)))
        max_cand = max(1, int(self.cfg.controller_max_candidates or 24))
        cand_show = candidates[:max_cand]

        lines: List[str] = []
        for i, c in enumerate(cand_show, start=1):
            sid = str(c.get("seed_id") or "")
            sc = float(c.get("score", 0.0))
            cost = int(c.get("cost_tokens", 0))
            step_idx = int(c.get("seed_step", -1))
            docids = c.get("seed_docids") or []
            docids_s = ",".join([str(d) for d in docids[:4]])
            preview = str(c.get("preview") or "")
            preview = preview.replace("\n", " ").strip()
            if len(preview) > 220:
                preview = preview[:220] + "..."
            lines.append(f"{i}. id={sid} score={sc:.4f} cost={cost} step={step_idx} docids={docids_s} | {preview}")

        sys = (
            "You are a memory-controller that selects which stored memory nodes to UNFOLD. "
            "You must return exactly one JSON object and nothing else."
        )

        user = (
            f"QUERY: {query}\n"
            f"BUDGET_UNFOLD_TOKENS: {int(budget_unfold)}\n"
            f"TARGET_K: {int(k)} (select <= {int(max_sel)} seed ids)\n\n"
            "Each candidate is a SEED node; selecting it will unfold its dependency closure.\n"
            "Pick a small set that likely contains the needed evidence with minimal cost.\n\n"
            "CANDIDATES:\n" + "\n".join(lines) + "\n\n"
            "Return JSON with the key select_seed_ids as a list of ids from the candidates.\n"
        )
        if bool(getattr(self.cfg, "controller_output_rationale", False)):
            user += "You may also include a short rationale string under key rationale.\n"
        user += "Example: {\"select_seed_ids\":[\"N_12\",\"N_51\"]}\n"

        raw = ""
        try:
            resp = self.controller_llm.generate([
                {"role": "system", "content": sys},
                {"role": "user", "content": user},
            ])
            raw = (resp.text or "").strip()
            self._accum_usage(getattr(resp, "usage", None))
            self.counters["controller_calls"] += 1
            try:
                if getattr(resp, "usage", None) and resp.usage.get("total_tokens") is not None:
                    self.counters["controller_total_tokens"] += int(resp.usage.get("total_tokens") or 0)
            except Exception:
                pass
        except Exception:
            self.counters["controller_errors"] += 1
            return [], True, raw

        obj_txt = _extract_first_json_object(raw or "")
        if not obj_txt:
            self.counters["controller_parse_fail"] += 1
            return [], True, raw

        try:
            obj = json.loads(obj_txt)
        except Exception:
            self.counters["controller_parse_fail"] += 1
            return [], True, raw

        ids = []
        if isinstance(obj, dict):
            for key in ("select_seed_ids", "seed_ids", "ids"):
                v = obj.get(key)
                if isinstance(v, list):
                    ids = [str(x).strip() for x in v if str(x).strip()]
                    break

        allowed = {str(c.get("seed_id")) for c in cand_show if c.get("seed_id")}
        picked: List[str] = []
        for sid in ids:
            if sid in allowed and sid not in picked:
                picked.append(sid)
            if len(picked) >= max_sel:
                break

        if not picked:
            self.counters["controller_empty_selection"] += 1
            return [], True, raw

        return picked, False, raw

    def _run_unfold(
        self,
        query: str,
        *,
        k: Optional[int],
        reason: str,
        step: int,
        task_id: str,
        method: str,
        run_tag: str,
        budget_override: Optional[int] = None,
    ) -> List[str]:
        """Run unfolding (optionally agentic) and emit a controller trace event."""
        if not hasattr(self.mem, "unfold"):
            return []

        prev_budget = getattr(self.mem, "budget_unfold", None)
        if budget_override is not None and prev_budget is not None:
            try:
                setattr(self.mem, "budget_unfold", int(budget_override))
            except Exception:
                pass

        activated: List[str] = []
        used_fallback = False
        raw = ""
        candidates: List[Dict[str, Any]] = []
        picked: List[str] = []

        try:
            # --- Bandit controller (GoC-Bandit) ---
            if (
                bool(getattr(self.cfg, "enable_bandit_unfold", False))
                and self.bandit_controller is not None
                and hasattr(self.mem, "compute_unfold_candidates")
                and hasattr(self.mem, "unfold_with_seed_ids")
                and str(method or "").lower().startswith("goc")
            ):
                kk = int(k or getattr(self.mem, "unfold_k", 6) or 6)
                try:
                    candidates = self.mem.compute_unfold_candidates(query, k=kk, topk=int(getattr(self.cfg, "controller_max_candidates", 24)))  # type: ignore[attr-defined]
                except Exception:
                    candidates = []
                bud = int(getattr(self.mem, "budget_unfold", 0) or 0)
                picked, dbg = self.bandit_controller.select_seed_ids(
                    candidates=candidates,
                    k=min(int(getattr(self.cfg, "controller_max_select", 6)), kk),
                    budget_unfold=bud,
                    now_step=int(step),
                    committed_titles=self._committed_supporting_titles,
                )
                if picked:
                    try:
                        activated = self.mem.unfold_with_seed_ids(query, picked, k=kk)  # type: ignore[attr-defined]
                        self.counters["bandit_unfold_calls"] += 1
                    except Exception:
                        activated = []
                        used_fallback = True
                else:
                    used_fallback = True

                if used_fallback:
                    activated = self.mem.unfold(query, kk)  # type: ignore[misc]

                # Trace bandit decision (compact)
                try:
                    compact = []
                    for c in (candidates or [])[: int(getattr(self.cfg, "controller_max_candidates", 24) or 24)]:
                        compact.append({
                            "seed_id": c.get("seed_id"),
                            "score": c.get("score"),
                            "cost_tokens": c.get("cost_tokens"),
                            "seed_step": c.get("seed_step"),
                            "seed_docids": (c.get("seed_docids") or [])[:4],
                            "preview": (c.get("preview") or "")[:240],
                        })
                    self._trace({
                        "type": "bandit_unfold",
                        "task_id": task_id,
                        "method": method,
                        "run_tag": run_tag,
                        "step": step,
                        "reason": reason,
                        "query": query,
                        "k": int(kk),
                        "budget_unfold": int(bud),
                        "picked_seed_ids": picked,
                        "used_fallback": bool(used_fallback),
                        "candidates": compact,
                        "bandit_debug": dbg,
                        "activated": (activated or [])[:25],
                    })
                except Exception:
                    pass

            # --- Option-B LLM controller (GoC-Agentic) ---
            elif (
                bool(getattr(self.cfg, "enable_agentic_unfold", False))
                and hasattr(self.mem, "compute_unfold_candidates")
                and hasattr(self.mem, "unfold_with_seed_ids")
                and str(method or "").lower().startswith("goc")
            ):
                kk = int(k or getattr(self.mem, "unfold_k", 6) or 6)
                try:
                    candidates = self.mem.compute_unfold_candidates(query, k=kk, topk=int(self.cfg.controller_max_candidates))  # type: ignore[attr-defined]
                except Exception:
                    candidates = []
                bud = int(getattr(self.mem, "budget_unfold", 0) or 0)
                picked, used_fallback, raw = self._controller_select_unfold_seed_ids(
                    query=query,
                    candidates=candidates,
                    k=kk,
                    budget_unfold=bud,
                    step=step,
                    task_id=task_id,
                    method=method,
                    run_tag=run_tag,
                    reason=reason,
                )
                if picked:
                    try:
                        activated = self.mem.unfold_with_seed_ids(query, picked, k=kk)  # type: ignore[attr-defined]
                    except Exception:
                        activated = []
                        used_fallback = True
                if (not picked) or used_fallback:
                    activated = self.mem.unfold(query, kk)  # type: ignore[misc]
            else:
                # Default (non-agentic)
                if k is None:
                    activated = self.mem.unfold(query)  # type: ignore[misc]
                else:
                    activated = self.mem.unfold(query, int(k))  # type: ignore[misc]
        finally:
            # restore budget
            if budget_override is not None and prev_budget is not None:
                try:
                    setattr(self.mem, "budget_unfold", prev_budget)
                except Exception:
                    pass

        # Drain memory-side events so folds/unfolds are visible in traces.
        try:
            self._drain_mem_events(task_id=task_id, method=method, run_tag=run_tag, step=step)
        except Exception:
            pass

        # Controller trace
        if bool(getattr(self.cfg, "enable_agentic_unfold", False)) and str(method or "").lower().startswith("goc"):
            try:
                # keep candidate payload compact
                compact = []
                for c in (candidates or [])[: int(self.cfg.controller_max_candidates or 24)]:
                    compact.append({
                        "seed_id": c.get("seed_id"),
                        "score": c.get("score"),
                        "cost_tokens": c.get("cost_tokens"),
                        "seed_step": c.get("seed_step"),
                        "seed_docids": (c.get("seed_docids") or [])[:4],
                        "preview": (c.get("preview") or "")[:240],
                    })
                self._trace({
                    "type": "controller_unfold",
                    "task_id": task_id,
                    "method": method,
                    "run_tag": run_tag,
                    "step": step,
                    "reason": reason,
                    "query": query,
                    "k": int(k or getattr(self.mem, "unfold_k", 6) or 6),
                    "budget_unfold": int(getattr(self.mem, "budget_unfold", 0) or 0),
                    "picked_seed_ids": picked,
                    "used_fallback": bool(used_fallback),
                    "candidates": compact,
                    "controller_raw": (raw or "")[:1200],
                    "activated": (activated or [])[:25],
                })
            except Exception:
                pass

        return activated

    def _maybe_adaptive_unfold(self, step: int, question: str, *, method: str, run_tag: str, task_id: str):
        """Controller-side adaptive unfolding.

        Motivation: GoC stores pruned nodes in `storage` but needs an *unfold policy* to
        recover relevant info. This keeps the policy lightweight and task-agnostic:
          - prefer unfolding when evidence coverage is low
          - allocate bigger unfold budget when coverage is low or we are stuck
        """
        if not self.cfg.adaptive_unfold:
            return
        if step < int(self.cfg.adaptive_unfold_min_step):
            return
        if self._adaptive_unfold_calls >= int(self.cfg.adaptive_unfold_max_calls):
            return
        if not hasattr(self.mem, "unfold"):
            return
        # Unfold is only meaningful when memory has storage/index (GoC), but safe to call otherwise.

        required = self._infer_required_fields(question)
        cov, missing = self._coverage_and_missing(required)

        # Trigger: low coverage OR recent loop/finish-block signals
        loopy = self._should_break_search_cycle("")  # based on current window
        blocked_finish = bool(self._last_finish_block_reason)
        if cov >= 0.85 and (not loopy) and (not blocked_finish):
            return

        # Choose unfold budget bucket
        buckets = list(self.cfg.adaptive_unfold_buckets or (0, 150, 300, 600, 900))
        buckets = sorted(set(int(b) for b in buckets if int(b) >= 0))
        if not buckets:
            buckets = [0]

        if cov < 0.25:
            target = max(buckets)
        elif cov < 0.5:
            target = max(b for b in buckets if b <= 600) if any(b <= 600 for b in buckets) else max(buckets)
        elif cov < 0.75:
            target = max(b for b in buckets if b <= 300) if any(b <= 300 for b in buckets) else max(buckets)
        else:
            target = max(b for b in buckets if b <= 150) if any(b <= 150 for b in buckets) else min(buckets)

        if loopy or blocked_finish:
            target = min(int(self.cfg.adaptive_unfold_budget_max), target + 150)
        target = min(int(self.cfg.adaptive_unfold_budget_max), int(target))

        # Pick unfold query: focus on the most missing (project, field) pair if available.
        uq = ""
        if missing:
            p, f = missing[0]
            uq = f"{p} {f}"
        else:
            uq = question

        # Temporarily adjust budget_unfold for this call.
        prev_budget = getattr(self.mem, "budget_unfold", None)
        try:
            if prev_budget is not None:
                setattr(self.mem, "budget_unfold", target)
            activated = self.mem.unfold(uq)  # type: ignore[attr-defined]
        except Exception:
            activated = []
        finally:
            if prev_budget is not None:
                try:
                    setattr(self.mem, "budget_unfold", prev_budget)
                except Exception:
                    pass

        if activated:
            self._adaptive_unfold_calls += 1
            self.counters["adaptive_unfold_calls"] += 1
            self.counters["adaptive_unfold_activated_nodes"] += int(len(activated))
            self._trace({
                "type": "adaptive_unfold",
                "task_id": task_id,
                "method": method,
                "run_tag": run_tag,
                "step": step,
                "query": uq,
                "budget": target,
                "coverage": cov,
                "activated": activated[:25],
            })


    def _policy_rewrite_query(self, query: str) -> str:
        """Cheap, generic query diversification when a query is blocked/cooldown."""
        q = (query or "").strip()
        if not q:
            return q
        # Add a generic intent token that tends to be present in profiles.
        # (This is not dataset-specific: many corpora contain 'profile', 'official', etc.)
        if "profile" not in q.lower():
            return q + " official profile"
        return q + " details"

    def _policy_preprocess_call(self, proposed_call: Dict[str, Any], step: int) -> Tuple[Optional[Dict[str, Any]], Optional[Dict[str, Any]]]:
        """Optionally block/override a tool call based on prior failures.

        Returns (call_to_execute or None, policy_log or None).
        If None is returned, the step is effectively blocked (we record a constraint and continue).
        """
        if not self.cfg.enable_failure_policy:
            return proposed_call, None

        tool = proposed_call.get("tool")
        args = (proposed_call.get("args") or {})

        # Preserve any LLM-declared GoC annotations across policy overrides.
        goc0 = proposed_call.get("goc") if isinstance(proposed_call, dict) else None
        def _attach_goc(call_dict: Dict[str, Any]) -> Dict[str, Any]:
            if isinstance(goc0, dict) and "goc" not in call_dict:
                call_dict["goc"] = goc0
            return call_dict

        # Track executed-tool streak after policy (we update streak for the *proposed* tool to detect habits)
        # We do not enforce on "finish" here; finish gating handles that.

        # Block duplicate open_page attempts (stronger than caching) to prevent thrashing.
        if tool == "open_page" and self.cfg.duplicate_open_hard_block:
            docid = args.get("docid")
            if docid and docid in self.opened_cache:
                self.counters["policy_overrides"] += 1
                self.counters["duplicate_open_blocked"] += 1
                cand = self._policy_candidate_docid()
                sig = self._sig_open_docid(docid)
                self._policy_record_failure_once(
                    key=f"fail_dup_open:{docid}",
                    text=(
                        f"[FAIL] Repeated open_page attempted for docid={docid} (already opened). "
                        "This pattern often causes thrashing; reuse cached FACTS and move to NEW evidence."
                    ),
                    docids=[sig, docid],
                )
                self._policy_record_constraint_once(
                    key=f"dup_open:{docid}",
                    text=(
                        f"[CONSTRAINT] Duplicate open_page blocked for docid={docid}. "
                        "Use cached FACTS from ACTIVE_CONTEXT and pick a NEW evidence docid instead."
                    ),
                    docids=[sig, docid],
                )
                if cand:
                    new_call = {"tool": "open_page", "args": {"docid": cand}}
                    new_call = _attach_goc(new_call)
                    return new_call, {"reason": "dup_open_override", "from": proposed_call, "to": new_call, "blocked_docid": docid, "opened_docid": cand}
                # No candidate -> allow (avoid deadlocks when return-gating expects open_page tool calls)
                # NOTE: open_page already dedupes exact view repeats via opened_view_cache.
                return proposed_call, {"reason": "dup_open_allow", "from": proposed_call, "docid": docid}

        # Enforce search cooldown for unproductive repeated queries.
        if tool == "search":
            query = args.get("query") or ""
            qnorm = self._normalize_query(query)

            # (v24) Candidate-first: if we were given an explicit candidate set, prefer
            # opening candidate evidence over global searches early on.
            if (
                self.cfg.candidate_first
                and self.cfg.candidate_first_override_global_search
                and self.given_projects
                and int(self.counters.get("open_page_calls", 0)) < int(self.cfg.candidate_first_min_open_pages)
                and self._is_globalish_search(query)
            ):
                cand = self._policy_candidate_docid()
                if cand:
                    self.counters["policy_overrides"] += 1
                    self.counters["candidate_first_overrides"] += 1
                    self._last_loop_escape_step = step
                    self._policy_record_constraint_once(
                        key="candidate_first_mode",
                        text=(
                            "[CONSTRAINT] Candidate-first mode: the question provides a candidate set. "
                            "Open candidate OFFICIAL PROFILE pages and extract FACTS before running global searches."
                        ),
                    )
                    new_call = {"tool": "open_page", "args": {"docid": cand}}
                    new_call = _attach_goc(new_call)
                    return new_call, {
                        "reason": "candidate_first_open",
                        "from": proposed_call,
                        "to": new_call,
                        "query": query,
                        "opened_docid": cand,
                    }

            # (v24) Cyclic loop breaker: if we detect low-entropy query alternation without new candidates,
            # force an evidence-opening action.
            if self._should_break_search_cycle(qnorm):
                cand = self._policy_candidate_docid()
                if cand:
                    self.counters["policy_overrides"] += 1
                    self.counters["search_cycle_break_overrides"] += 1
                    self._last_loop_escape_step = step
                    self._policy_record_constraint_once(
                        key="search_cycle_breaker",
                        text=(
                            "[CONSTRAINT] Detected a cyclic search loop (repeating a small set of queries with no new evidence). "
                            "Stop searching and open new evidence documents instead."
                        ),
                    )
                    new_call = {"tool": "open_page", "args": {"docid": cand}}
                    new_call = _attach_goc(new_call)
                    return new_call, {
                        "reason": "search_cycle_break_open",
                        "from": proposed_call,
                        "to": new_call,
                        "query": query,
                        "opened_docid": cand,
                    }

            exp = self._blocked_queries.get(qnorm)
            if exp is not None and step < exp:
                self.counters["policy_overrides"] += 1
                self.counters["blocked_search_query"] += 1
                cand = self._policy_candidate_docid()
                self._policy_record_constraint_once(
                    key=f"blocked_query:{qnorm}:{exp}",
                    text=(
                        f"[CONSTRAINT] Search query on cooldown (too many no-progress repeats): '{query}'. "
                        "Use a different query or open a new evidence document instead."
                    ),
                    docids=[self._sig_search_docid(query)],
                )
                if cand:
                    new_call = {"tool": "open_page", "args": {"docid": cand}}
                    new_call = _attach_goc(new_call)
                    return new_call, {"reason": "search_cooldown_override", "from": proposed_call, "to": new_call, "query": query, "opened_docid": cand}
                # Rewrite the query as a soft fallback
                new_q = self._policy_rewrite_query(query)
                new_call = {"tool": "search", "args": {"query": new_q, "topk": args.get("topk", 10)}}
                new_call = _attach_goc(new_call)
                return new_call, {"reason": "search_cooldown_rewrite", "from": proposed_call, "to": new_call, "query": query, "rewritten_query": new_q}

        # If we are stuck repeating the same executed tool with no progress, force a mode switch.
        # "Progress" is defined as having opened a new docid or having produced a search with unseen candidates.
        predicted_streak = (self._exec_tool_streak + 1) if (self._last_exec_tool and tool == self._last_exec_tool) else 1
        no_progress_for = step - int(self._last_progress_step or 0)
        if predicted_streak >= int(self.cfg.max_consecutive_same_tool) and no_progress_for >= 2:
            cand = self._policy_candidate_docid()
            if cand and tool != "open_page":
                self.counters["policy_overrides"] += 1
                self.counters["mode_switch_overrides"] += 1
                self._policy_record_constraint_once(
                    key=f"mode_switch:{step}:{tool}",
                    text=(
                        "[CONSTRAINT] Detected repeated actions with no progress. "
                        "Switch to opening NEW evidence documents and extracting FACTS before further searching."
                    ),
                )
                new_call = {"tool": "open_page", "args": {"docid": cand}}
                new_call = _attach_goc(new_call)
                return new_call, {"reason": "mode_switch_open_page", "from": proposed_call, "to": new_call, "opened_docid": cand, "streak": predicted_streak}

        return proposed_call, None

    def _policy_post_search(self, query: str, result_docids: List[str], step: int):
        """Update unproductive-search counters and set query cooldowns."""
        if not self.cfg.enable_failure_policy:
            return
        qnorm = self._normalize_query(query)
        new_candidates = [d for d in (result_docids or []) if d and d not in self.opened_cache]

        # Track recent search entropy / novelty for cyclic-loop detection.
        try:
            self._recent_search_norms.append(qnorm)
            self._recent_search_newcand.append(1 if new_candidates else 0)
        except Exception:
            pass

        if new_candidates:
            # progress: we have at least one unopened candidate
            self._last_progress_step = step
            # Clear any previous finish-block signal; we are making forward progress.
            self._last_finish_block_reason = None
            # reset unproductive count (reward new evidence)
            if qnorm in self._unproductive_queries:
                self._unproductive_queries[qnorm] = 0
            return

        # No new candidates from this query: record as failure signature
        self._unproductive_queries[qnorm] += 1
        self.counters["unproductive_searches"] += 1

        sig = self._sig_search_docid(query)
        if self._unproductive_queries[qnorm] == 1:
            self._policy_record_failure_once(
                key=f"fail_search_nonew:{qnorm}",
                text=(
                    f"[FAIL] Search returned no new (unopened) evidence candidates for query: '{query}'. "
                    "Repeating the same search is unlikely to help; change the query or open a new docid."
                ),
                docids=[sig],
            )

        if self._unproductive_queries[qnorm] >= int(self.cfg.unproductive_search_threshold):
            # Cooldown this query to prevent repetition
            exp = step + int(self.cfg.block_search_ttl_steps)
            self._blocked_queries[qnorm] = exp
            self.counters["search_queries_cooled_down"] += 1
            self._policy_record_failure_once(
                key=f"fail_search_cooldown:{qnorm}:{exp}",
                text=(
                    f"[FAIL] Search entered cooldown after repeated no-progress attempts: '{query}'. "
                    "This is a strong signal to stop repeating this query and switch strategy."
                ),
                docids=[sig],
            )
            self._policy_record_constraint_once(
                key=f"cooldown:{qnorm}:{exp}",
                text=(
                    f"[CONSTRAINT] Query produced no new evidence repeatedly; cooling down: '{query}'. "
                    "Use a different query OR open new evidence docids instead of repeating this search."
                ),
                docids=[sig],
            )


    def _extract_given_projects(self, question: str) -> List[str]:
        """Extract ordered unique Project_#### mentions from the question."""
        seen = set()
        out: List[str] = []
        for mm in re.finditer(r"\bProject_\d{4}\b", question or ""):
            p = mm.group(0)
            if p not in seen:
                seen.add(p)
                out.append(p)
        return out

    def _next_unopened_given_docid(self) -> Optional[str]:
        """Return next D_TRUTH_#### for an unopened Project_#### in the question."""
        while self._given_projects_open_idx < len(self.given_projects):
            p = self.given_projects[self._given_projects_open_idx]
            self._given_projects_open_idx += 1
            did = f"D_TRUTH_{p.split('_')[1]}"
            if did not in self.opened_cache:
                return did
        return None

    def _make_fact_header(self, content: str, max_chars: int) -> str:
        """Extract a compact FACTS header from an OFFICIAL PROFILE page."""
        if not content:
            return ""
        fields: Dict[str, str] = {}

        m_proj = re.search(r"\bProject_\d{4}\b", content)
        if m_proj:
            fields["project"] = m_proj.group(0)

        m_year = re.search(r"(?im)^start_year\s*:\s*(\d{4})\s*$", content)
        if m_year:
            fields["start_year"] = m_year.group(1)

        m_hq = re.search(r"(?im)^headquarters\s*:\s*([^\n]+)$", content)
        if m_hq:
            fields["headquarters"] = m_hq.group(1).strip()

        m_code = re.search(r"(?im)^code_name\s*:\s*([^\n]+)$", content)
        if m_code:
            fields["code_name"] = m_code.group(1).strip()

        m_key = re.search(r"(?im)^key_number\s*:\s*(\d+)\s*$", content)
        if m_key:
            fields["key_number"] = m_key.group(1)

        m_rel = re.search(r"(?im)^related_projects\s*:\s*([^\n]+)$", content)
        if m_rel:
            fields["related_projects"] = m_rel.group(1).strip()

        if not fields:
            return ""

        parts = [f"{k}={v}" for k, v in fields.items()]
        header = "FACTS: " + " | ".join(parts)
        if len(header) > max_chars:
            header = header[:max_chars].rstrip()
        return header

    def _open_view_key(self, docid: Optional[str], url: Optional[str], args: Dict[str, Any]) -> str:
        """Stable key for deduping *views* of an opened page.

        We intentionally allow reopening the same doc with a different view
        (e.g., head vs tail vs find(...) snippets) while still deduping
        exact repeats to avoid wasting active context.
        """
        section = (args.get("section") or "head")
        find = (args.get("find") or "")
        offset = args.get("offset")
        max_chars = args.get("max_chars")
        find_window = args.get("find_window")
        # Keep it short but unique.
        return "|".join([
            f"docid={docid or ''}",
            f"url={url or ''}",
            f"section={section}",
            f"find={find}",
            f"offset={'' if offset is None else int(offset)}",
            f"max_chars={'' if max_chars is None else int(max_chars)}",
            f"find_window={'' if find_window is None else int(find_window)}",
        ])

    def _select_open_page_view(self, full_content: str, args: Dict[str, Any]) -> Dict[str, Any]:
        """Create a *view* of a page (head/tail/offset/find) for prompt memory."""
        content = full_content or ""
        section = (args.get("section") or "head").lower()
        find = args.get("find")

        # Keep views reasonably small; we still apply a final 2500-char cap later.
        max_chars = int(args.get("max_chars", 1800))
        max_chars = max(200, min(6000, max_chars))

        offset = args.get("offset")
        if offset is not None:
            try:
                off = max(0, int(offset))
            except Exception:
                off = 0
            view = content[off: off + max_chars]
            return {"view": view, "tag": f"[VIEW offset={off} max_chars={max_chars}]"}

        if find:
            pat = str(find)
            hay_low = content.lower()
            pat_low = pat.lower()
            idx = hay_low.find(pat_low)
            win = int(args.get("find_window", 800))
            win = max(200, min(4000, win))
            if idx >= 0:
                start = max(0, idx - win // 2)
                end = min(len(content), idx + len(pat) + win // 2)
                # Expand to line boundaries when possible.
                lb = content.rfind("\n", 0, start)
                if lb >= 0:
                    start = lb + 1
                rb = content.find("\n", end)
                if rb >= 0:
                    end = rb
                view = content[start:end]
                return {"view": view, "tag": f"[FIND '{pat}' FOUND]"}
            # Not found: fall back to tail since tasks often hide the line deep.
            section = "tail"
            not_found_tag = f"[FIND '{pat}' NOT FOUND â†’ showing tail]"
            # continue into tail logic with this tag
            if section == "tail":
                view = content[-max_chars:] if len(content) > max_chars else content
                return {"view": view, "tag": not_found_tag}

        if section == "tail":
            view = content[-max_chars:] if len(content) > max_chars else content
            return {"view": view, "tag": f"[VIEW tail max_chars={max_chars}]"}

        # default: head
        view = content[:max_chars]
        return {"view": view, "tag": f"[VIEW head max_chars={max_chars}]"}

    def _authority_prefix(self, full_content: str) -> str:
        """Heuristic authority hint to reduce distractor evidence without docid-based cheating."""
        first = (full_content or "").splitlines()[:1]
        first_line = first[0] if first else ""
        if "OFFICIAL PROFILE" in first_line:
            return "SOURCE: OFFICIAL PROFILE (authoritative)"
        return "WARNING: NOT OFFICIAL PROFILE TEMPLATE (low-authority; may be distractor)."

    def _compose_open_page_observation(self, content: str) -> str:
        """Observation text stored into memory for open_page."""
        max_len = 2500
        body = content or ""
        if not self.cfg.open_page_fact_header:
            return body[:max_len]

        header = self._make_fact_header(body, max_chars=int(self.cfg.fact_header_max_chars))
        if not header:
            return body[:max_len]

        reserve = len(header) + 2  # header + blank line
        body_snip = body[: max(0, max_len - reserve)]
        return header + "\n\n" + body_snip

    def _compose_open_page_observation_view(self, full_content: str, view_content: str, prefix: str, tag: str) -> str:
        """Compose the open_page observation stored in ACTIVE_CONTEXT.

        - full_content is stored losslessly in storage; used for FACT HEADER extraction.
        - view_content is what we show the model (head/tail/find snippet).
        """
        max_len = 2500
        lines: List[str] = []
        if prefix:
            lines.append(prefix)
        if tag:
            lines.append(tag)

        header = ""
        if self.cfg.open_page_fact_header:
            header = self._make_fact_header(full_content or "", max_chars=int(self.cfg.fact_header_max_chars))
            if header:
                lines.append(header)

        # Separator before body
        if lines:
            lines.append("")

        body = view_content or ""
        # Compute remaining budget
        pre = "\n".join(lines)
        rem = max(0, max_len - len(pre))
        body_snip = body[:rem]
        return (pre + body_snip)[:max_len]

    def _should_block_finish(self, reason: str) -> bool:
        """Block finish at most N times per reason to avoid infinite loops."""
        max_blocks = int(getattr(self.cfg, "max_finish_blocks_per_reason", 1) or 1)
        if self._finish_block_counts[reason] >= max_blocks:
            return False
        self._finish_block_counts[reason] += 1
        return True

    # --- GoC annotation prompt gating (token-saving) ---
    def _goc_stage_tag(self, user_prompt: str) -> str:
        """Best-effort stage tagging for two-stage benchmarks.

        NOTE: This is intentionally heuristic (string matching) and is used ONLY
        to decide whether to inject a short GoC-annotation hint line.
        """
        up = (user_prompt or "")
        low = up.lower()

        # Merge turns (multi-commit DAG)
        if "[merge" in low:
            return "merge"

        # Stage-2 / FINAL signals (be careful: initial prompts often contain
        # "Do NOT call finish"; we must not mis-tag those as stage2).
        if ("now call finish" in low) or ("now call `finish`" in low):
            return "stage2"
        if (
            ("follow-up" in low or "late-binding" in low)
            and ("call finish" in low or "call `finish`" in low)
            and ("do not call finish" not in low)
        ):
            return "stage2"
        if ("finish.args" in low or "finish.args.answer" in low) and ("do not call finish" not in low):
            return "stage2"
        if ("/ final" in low) or ("[final" in low) or ("[follow-up 2" in low):
            return "stage2"

        # Stage-1 / COMMIT signals
        if ("/ commit" in low) or ("[commit" in low) or ("[follow-up 1" in low):
            return "stage1"
        if ("commit" in low) and ("supporting_titles" in low or "supporting titles" in low or "return" in low):
            return "stage1"

        return "other"

    def _should_inject_goc_hint(self, *, step: int, remaining: int, current_user_prompt: str) -> bool:
        mode = str(getattr(self.cfg, "goc_annotation_mode", "none") or "none").lower().strip()
        if mode in ("none", "off", "false", "0"):
            return False
        gate = str(getattr(self.cfg, "goc_annotation_gate", "") or "").lower()
        if not gate:
            return False
        toks = {t.strip() for t in re.split(r"[\s,;]+", gate) if t.strip()}
        if not toks:
            return False
        if "always" in toks:
            return True

        # IMPORTANT: gates are OR'ed.
        # (Bugfix) Previously, including "stage2" in the gate list would
        # suppress other triggers (e.g., doc_switch) on non-stage2 steps.
        st = self._goc_stage_tag(current_user_prompt)
        triggered = False

        if "stage2" in toks and st == "stage2":
            if not getattr(self, "_goc_stage2_hint_sent", False):
                triggered = True
        if "stage1" in toks and st == "stage1":
            if not getattr(self, "_goc_stage1_hint_sent", False):
                triggered = True

        if "doc_switch" in toks and self._goc_doc_switch_pending:
            triggered = True

        if "commit_turn" in toks and st == "stage1":
            triggered = True

        if "merge_turn" in toks and st == "merge":
            triggered = True

        if "pre_finish" in toks:
            k = int(getattr(self.cfg, "goc_annotation_gate_pre_finish_steps", 2) or 2)
            if remaining <= max(1, k):
                triggered = True

        if "every_k" in toks:
            k = int(getattr(self.cfg, "goc_annotation_gate_every_k_steps", 0) or 0)
            if k > 0 and ((step + 1) % k == 0):
                triggered = True

        return triggered
    def _goc_hint_line(self, stage_tag: str = "other") -> str:
        """Single compact hint line appended on gated steps.

        Goal: maximize compliance while keeping token overhead minimal.
        """
        mode = str(getattr(self.cfg, "goc_annotation_mode", "none") or "none").lower().strip()
        schema = str(getattr(self.cfg, "goc_annotation_schema", "compact") or "compact").lower().strip()
        force_val = getattr(self.cfg, "goc_annotation_force", 1)
        try:
            force_level = int(force_val)
        except Exception:
            force_level = 1 if bool(force_val) else 0
        force = bool(force_level)

        # Keep this *very* short: this line is injected only on gated steps.
        if schema == "legacy":
            core = '"goc": {"depends_on_steps": [<prev_step_ints>]}'
            ex = '{"tool":"search","args":{"query":"..."},"goc":{"depends_on_steps":[1]}}'
        else:
            core = '"goc": {"d": [-1,-2], "c": [1|2]}'
            ex = '{"tool":"open_page","args":{"docid":"..."},"goc":{"d":[-1]}}'

        lead = "MUST add" if force else "If relevant, add"
        base = (
            f'{lead} top-level {core}. Keep "tool"/"args" unchanged. '
            'Prefer goc:{"d":[-1]} if unsure. '
            f'Example: {ex}'
        )

        # Stage-aware micro-guidance (kept short): in HotpotQA two-stage setups,
        # we want "c" to capture which committed title (1/2) the step relies on.
        if (schema != "legacy") and (stage_tag == "stage2"):
            base += ' Stage2: if relying on committed anchors, set goc:{"c":[...]} (title index or commit idx). '
        if (schema != "legacy") and (stage_tag == "merge"):
            base += ' Merge: if comparing commits, set goc:{"c":[i,j]}. '
        if force_level >= 2:
            base += ' Do NOT use goc:{}; if unsure use goc:{"d":[-1]}.'
        if mode == "tracefirst":
            base += ' ; optional goc:{"step_notes":["..."]}.'
        return base

    def _build_system_prompt(self) -> str:
        base = (
            "You are an assistant that MUST use the provided tools.\n"
            "You MUST output exactly ONE JSON object per turn. No extra text.\n"
            "Required keys: \"tool\" and \"args\". You may add extra top-level keys when instructed (e.g., \"goc\").\n"
            "Tools available: search, open_page, branch, return, finish.\n"
            "\n"
            "Critical rules (avoid invalid / wasteful calls):\n"
            " - Do NOT call `finish` until you have gathered evidence: at least 1 `open_page` and at least 1 docid cited.\n"
            " - The `finish.args.explanation` MUST include the evidence docids you used (e.g., 'Evidence docids: D_TRUTH_0001').\n"
            " - Do NOT call `finish` if you have not opened any page yet. Call search -> open_page first.\n"
            " - `return` usage: (a) inside a branch to exit that branch, OR (b) in MAIN to request the next user follow-up if the task is multi-turn.\n"
            " - When you call `branch`, you MUST include args: {description, prompt}.\n"
            " - When you call `return`, you MUST include args: {message}.\n"
            " - Call `return` at most ONCE per branch. Never call return twice in a row.\n"
            " - For this benchmark, branching is optional; prefer search/open_page unless a sub-task is clearly needed.\n"
            "\n"
            "Guidance:\n"
            " - Use search -> open_page to gather evidence docids.\n"
            " - `open_page` supports optional args for deep facts: {section: 'head'|'tail', find: '<pattern>', offset: <int>, max_chars: <int>, find_window: <int>}.\n"
            "   Example: {\"tool\":\"open_page\",\"args\":{\"docid\":\"D_TRUTH_0001\",\"find\":\"relocation_note\"}}\n"
            "   Or: {\"tool\":\"open_page\",\"args\":{\"docid\":\"D_TRUTH_0001\",\"section\":\"tail\"}}\n"
            " - Always cite evidence by including docids in explanation.\n"
        )

        # NOTE: we intentionally keep annotation instructions out of the system prompt
        # to minimize token overhead and avoid adding cognitive load on every step.
        # When enabled, we add a single short hint line only on gated steps.
        mode = str(getattr(self.cfg, "goc_annotation_mode", "none") or "none").lower().strip()
        if mode in ("hybrid_depends", "tracefirst"):
            base += (
                "\n(Research) If asked, include top-level \"goc\" in the same JSON output (NOT inside args).\n"
            )
        return base


    def _estimate_prompt_tokens(self, messages: List[Dict[str, str]]) -> Dict[str, Any]:
        """Estimate token usage by coarse prompt components (heuristic: chars/4).

        This is NOT model-tokenizer exact, but is stable enough to compare methods and to
        detect when ACTIVE_CONTEXT dominates prompt size.
        """
        est = {
            "total": 0,
            "system": 0,
            "user_base": 0,
            "followups": 0,
            "active_context_total": 0,
            "active_tool_lines": 0,
            "active_obs_lines": 0,
            "active_noise_lines": 0,
            "active_summary_lines": 0,
            "active_meta_lines": 0,
            "active_other_lines": 0,
            "other_roles": 0,
        }
        for m in messages:
            role = m.get("role", "")
            content = m.get("content", "") or ""
            t = approx_token_count(content)
            est["total"] += t
            if role == "system":
                est["system"] += t
                continue
            if role != "user":
                est["other_roles"] += t
                continue

            # user role
            if content.startswith("ACTIVE_CONTEXT:"):
                est["active_context_total"] += t
                # Sub-breakdown: line tags inside ACTIVE_CONTEXT
                try:
                    for line in content.splitlines()[1:]:
                        lt = approx_token_count(line)
                        if not line:
                            continue
                        if line.startswith("[NOISE/"):
                            est["active_noise_lines"] += lt
                        elif line.startswith("[TOOL:"):
                            est["active_tool_lines"] += lt
                        elif line.startswith("obs="):
                            est["active_obs_lines"] += lt
                        elif line.startswith("[RAG_RECALL") or line.startswith("[SUMMARY") or line.startswith("[LINEAR_SUMMARY") or line.startswith("[FOLD_PROXY"):
                            est["active_summary_lines"] += lt
                        elif line.startswith("[SYSTEM]") or line.startswith("[ASSISTANT]") or line.startswith("[FAILURE"):
                            est["active_meta_lines"] += lt
                        else:
                            est["active_other_lines"] += lt
                except Exception:
                    pass
            else:
                # Follow-up user messages are typically bracketed.
                if content.startswith("[FOLLOW-UP"):
                    est["followups"] += t
                else:
                    est["user_base"] += t
        return est

    def _accum_prompt_est(self, est: Dict[str, Any]):
        """Accumulate prompt estimates into counters for per-task tool_stats."""
        try:
            self.counters["prompt_est_total"] += int(est.get("total", 0))
            self.counters["prompt_est_system"] += int(est.get("system", 0))
            self.counters["prompt_est_user_base"] += int(est.get("user_base", 0))
            self.counters["prompt_est_followups"] += int(est.get("followups", 0))
            self.counters["prompt_est_active_context_total"] += int(est.get("active_context_total", 0))
            self.counters["prompt_est_active_noise_lines"] += int(est.get("active_noise_lines", 0))
            self.counters["prompt_est_active_tool_lines"] += int(est.get("active_tool_lines", 0))
            self.counters["prompt_est_active_obs_lines"] += int(est.get("active_obs_lines", 0))
            self.counters["prompt_est_active_summary_lines"] += int(est.get("active_summary_lines", 0))
            self.counters["prompt_est_active_meta_lines"] += int(est.get("active_meta_lines", 0))
            self.counters["prompt_est_active_other_lines"] += int(est.get("active_other_lines", 0))
        except Exception:
            pass

    def _drain_mem_events(self, task_id: str, method: str, run_tag: str, step: int):
        """Drain MemoryManager event buffer and log/accumulate stats."""
        if not hasattr(self.mem, "drain_events"):
            return
        try:
            evs = self.mem.drain_events()  # type: ignore[attr-defined]
        except Exception:
            return
        if not evs:
            return
        for ev in evs:
            ev_type = (ev or {}).get("type", "unknown")
            # Trace
            try:
                self._trace({
                    "type": "mem_event",
                    "task_id": task_id,
                    "method": method,
                    "run_tag": run_tag,
                    "step": step,
                    "ev_type": ev_type,
                    "event": ev,
                })
            except Exception:
                pass

            # Aggregate
            try:
                if ev_type == "fold":
                    self.counters["mem_fold_events"] += 1
                    self.counters["mem_fold_removed_tokens_est"] += int(ev.get("chunk_tokens", 0))
                    self.counters["mem_fold_overflow_tokens_est"] += int(ev.get("overflow_tokens", 0))
                    self.counters["mem_fold_chunk_nodes"] += int(ev.get("chunk_size", 0))
                elif ev_type == "unfold":
                    self.counters["mem_unfold_events"] += 1
                    self.counters["mem_unfold_added_tokens_est"] += int(ev.get("added_tokens", ev.get("used_tokens_est", 0)) or 0)
                    self.counters["mem_unfold_activated_nodes"] += int(ev.get("activated_count", len(ev.get("activated", []) or [])) or 0)
                elif ev_type == "rag_unfold":
                    self.counters["mem_rag_unfold_events"] += 1
                    self.counters["mem_rag_unfold_activated_nodes"] += int(len(ev.get("activated", []) or []))
                elif ev_type in ("pef_fold", "pef_roll_fold"):
                    self.counters[f"mem_{ev_type}_events"] += 1
                    self.counters[f"mem_{ev_type}_episode_nodes"] += int(ev.get("episode_nodes", ev.get("rolled_nodes", 0)))
            except Exception:
                pass

    def _json_recovery_prompt(self) -> str:
        return (
            "Your previous message was invalid. Output exactly ONE JSON object and NOTHING else.\n"
            "Valid examples:\n"
            "  {\\\"tool\\\":\\\"search\\\",\\\"args\\\":{\\\"query\\\":\\\"Project_0001 headquarters\\\",\\\"topk\\\":5}}\n"
            "  {\\\"tool\\\":\\\"open_page\\\",\\\"args\\\":{\\\"docid\\\":\\\"D_TRUTH_0001\\\"}}\n"
            "  {\\\"tool\\\":\\\"finish\\\",\\\"args\\\":{\\\"answer\\\":\\\"...\\\",\\\"explanation\\\":\\\"Evidence docids: D_TRUTH_0001\\\",\\\"confidence\\\":\\\"80%\\\"}}\n"
            "Now output a corrected JSON object for your next tool call."
        )

    def _call_model_for_json(self, messages: List[Dict[str, str]], step: int) -> Tuple[Optional[Dict[str, Any]], List[Dict[str, Any]]]:
        """Call model, parse JSON, retry with recovery prompt if needed.

        Returns: (tool_call or None, attempt_logs)
        attempt_logs: list of {attempt_idx, raw_output, parsed_ok}
        """
        attempt_logs: List[Dict[str, Any]] = []

        resp = self.llm.generate(messages=messages, tools=None)
        self._accum_usage(getattr(resp, "usage", None))
        raw = (resp.text or "")
        call = parse_json_tool_call(raw)
        attempt_logs.append({
            "attempt": 0,
            "raw_output": raw[: self.cfg.log_output_chars],
            "parsed_ok": call is not None,
            "usage": getattr(resp, "usage", None)
        })
        if call is not None:
            return call, attempt_logs

        self.counters["json_parse_failures"] += 1

        for retry in range(1, self.cfg.max_json_retries + 1):
            # Minimal recovery messages to avoid growing prompt.
            recovery_messages = messages + [{"role": "user", "content": self._json_recovery_prompt()}]
            r2 = self.llm.generate(messages=recovery_messages, tools=None)
            self._accum_usage(getattr(r2, "usage", None))
            raw2 = (r2.text or "")
            call2 = parse_json_tool_call(raw2)
            attempt_logs.append({
                "attempt": retry,
                "raw_output": raw2[: self.cfg.log_output_chars],
                "parsed_ok": call2 is not None,
                "usage": getattr(r2, "usage", None)
            })
            if call2 is not None:
                self.counters["json_recoveries"] += 1
                return call2, attempt_logs

        return None, attempt_logs

    def run(
        self,
        user_question: str,
        *,
        user_turns: Optional[List[str]] = None,
        task_meta: Optional[Dict[str, Any]] = None,
        task_id: str = "task",
        method: str = "method",
        run_tag: str = "run",
    ) -> Dict[str, Any]:
        """Run one task.

        task_id/method/run_tag are used for logging and stdout progress only.
        """
        self.mem.reset()
        self.usage_accum = {"total_tokens": 0, "input_tokens": 0, "output_tokens": 0}
        self.counters = Counter()
        self.search_query_counts = Counter()
        self.evidence_docids = []
        self.opened_cache = {}
        self._llm_step_to_tool_nid = {}
        self._tracefirst_note_seq = 0
        # Multi-turn support: allow the environment to deliver follow-up user turns.
        turns: List[str] = [user_question]
        if user_turns and isinstance(user_turns, list) and user_turns:
            turns = [str(x) for x in user_turns if x is not None]
            if not turns:
                turns = [user_question]
        current_user_prompt = turns[0]
        pending_user_turns: List[str] = turns[1:]
        self.counters["pending_user_turns_init"] = len(pending_user_turns)

        # Per-turn return gating (optional): track last MAIN return so gating can reset each follow-up.
        self.counters["main_return_last_step"] = 0
        self.counters["main_return_last_open_pages"] = 0
        self.counters["main_return_last_open_page_tool_calls"] = 0
        self.counters["open_page_tool_calls_main"] = 0
        # Reset per-task commit state
        self._committed_supporting_titles = None
        self._committed_supporting_titles_by_commit = {}
        self._committed_answers_by_commit = {}
        self._goc_commit_anchor_nids_by_commit = {}
        self._goc_commit_title_nids_by_commit = {}
        self._goc_commit_anchor_nid = None
        self._goc_commit_title_nids = {}
        self._stage_final_unfold_done = False
        self._stage_commit_unfold_done = False
        self._stage_commit_unfold_done_for = set()
        self._goc_doc_switch_pending = False
        self._goc_last_open_docid = None
        self._goc_stage1_hint_sent = False
        self._goc_stage2_hint_sent = False
        self._correction_sent = set()
        # Extract Q1 for multi-turn benchmarks that embed it in the initial prompt.
        # This is intentionally lightweight: it never affects single-turn tasks.
        self._q1_text = ""
        try:
            for ln in str(current_user_prompt).splitlines():
                if ln.strip().startswith("Q1:"):
                    self._q1_text = ln.split("Q1:", 1)[1].strip()
                    break
        except Exception:
            self._q1_text = ""

        # Per-task policy overrides (benchmark-encoded levers).
        task_meta = task_meta or {}
        # GoC folding policy overrides (optional; set via bench_kwargs).
        try:
            pol = task_meta.get("goc_fold_policy") or task_meta.get("fold_policy")
            if pol and hasattr(self.mem, "fold_policy"):
                setattr(self.mem, "fold_policy", str(pol))
            _knobs = {
                "goc_dfs_hi_mult": ("dfs_hi_mult", float),
                "goc_dfs_lo_mult": ("dfs_lo_mult", float),
                "goc_dfs_roll_keep_last": ("dfs_roll_keep_last", int),
                "goc_dfs_roll_min_chunk": ("dfs_roll_min_chunk", int),
                "goc_dfs_switch_keep_last": ("dfs_switch_keep_last", int),
                "goc_dfs_phase_keep_last": ("dfs_phase_keep_last", int),
                "goc_pef_hi_mult": ("pef_hi_mult", float),
                "goc_pef_lo_mult": ("pef_lo_mult", float),
                "goc_pef_roll_keep_last": ("pef_roll_keep_last", int),
                "goc_pef_roll_min_chunk": ("pef_roll_min_chunk", int),
            }
            for k, (attr, cast) in _knobs.items():
                if k in task_meta and hasattr(self.mem, attr):
                    try:
                        setattr(self.mem, attr, cast(task_meta[k]))
                    except Exception:
                        pass
        except Exception:
            pass
        # Multi-turn injection thresholds can be overridden per task.
        mt_min_step = int(task_meta.get("multi_turn_min_step", self.cfg.multi_turn_min_step))
        mt_min_open_pages = int(task_meta.get("multi_turn_min_open_pages", self.cfg.multi_turn_min_open_pages))
        # Return gating prevents prematurely requesting follow-ups and collapsing the horizon.
        rg_min_steps = int(task_meta.get("return_gating_min_steps", mt_min_step))
        rg_min_open_pages = int(task_meta.get("return_gating_min_open_pages", mt_min_open_pages))
        # Closed-book final stage (benchmark-controlled). Active only when the user prompt includes a marker.
        closed_book_final = bool(task_meta.get("closed_book_final", False))
        # Disable auto-injected follow-ups for commit-based benchmarks (two-stage / multi-commit DAG).
        # In these settings, the next user turn MUST arrive via an explicit `return` call to preserve
        # fairness and stage semantics.
        commit_flow = bool(task_meta.get("two_stage", False) or task_meta.get("multi_commit", False))
        try:
            if int(task_meta.get("multi_commit_n", 0) or 0) >= 2:
                commit_flow = True
        except Exception:
            pass
        auto_inject_enabled = bool(self.cfg.multi_turn_auto_inject) and (not commit_flow)


        # Benchmark augmentation: inject internal "noise" nodes after stage-1 commit to increase
        # context pressure WITHOUT adding extra LLM calls. Applies to all methods equally.
        noise_nodes_after_stage1 = int(task_meta.get("noise_nodes_after_stage1", 0))
        noise_node_chars = int(task_meta.get("noise_node_chars", 320))
        noise_seed = int(task_meta.get("noise_seed", 7))
        self._noise_injected_after_stage1 = False
        # Optional: noise injection after EVERY commit (multi-commit / DAG benches).
        noise_nodes_after_commit = int(task_meta.get("noise_nodes_after_commit", 0))
        self._noise_injected_after_commit: set = set()

        def _inject_noise_nodes(kind: str, count: int):
            """Inject deterministic, task-scoped noise into memory.

            We record as regular msg nodes (NOT summary), so GoC does not anchor-protect them.
            This provides fair context pressure across methods while allowing better folding policies
            to discard noise.
            """
            if count <= 0:
                return
            try:
                import hashlib
                import random
                # Stable per-task salt
                sid = int(hashlib.md5(str(task_id).encode("utf-8")).hexdigest()[:8], 16)
                rng = random.Random(int(noise_seed) + sid)
            except Exception:
                rng = None

            words = [
                "archive", "protocol", "committee", "baseline", "variance", "notation", "dataset",
                "appendix", "footnote", "chronicle", "specimen", "parliament", "municipal",
                "cartography", "thermodynamics", "folklore", "astronomy", "linguistics",
                "taxonomy", "orchestra", "hydrology", "calculus", "jurisdiction",
                "biography", "metallurgy", "philology", "conjecture", "emphasis",
            ]

            def make_para(target_chars: int) -> str:
                if target_chars <= 40:
                    target_chars = 40
                out = []
                cur = 0
                # Build 3-6 short sentences.
                n_sent = 4
                if rng is not None:
                    n_sent = rng.randint(3, 6)
                for _ in range(n_sent):
                    w = []
                    if rng is not None:
                        k = rng.randint(10, 18)
                        for _i in range(k):
                            w.append(words[rng.randint(0, len(words) - 1)])
                    else:
                        w = words[:12]
                    sent = " ".join(w).capitalize() + "."
                    out.append(sent)
                    cur += len(sent) + 1
                    if cur >= target_chars:
                        break
                txt = " ".join(out)
                # Pad to target length with a repeating tail if needed.
                if len(txt) < target_chars:
                    tail = " Note: this paragraph is intentionally irrelevant." 
                    while len(txt) < target_chars:
                        txt += tail
                return txt[:target_chars]

            for i in range(int(count)):
                txt = make_para(int(noise_node_chars))
                self.mem.record_msg(f"[NOISE/{kind}/{i+1}/{count}] {txt}")
            self._trace({
                "type": "noise_injected",
                "task_id": task_id,
                "method": method,
                "run_tag": run_tag,
                "step": self._last_progress_step,
                "kind": kind,
                "count": int(count),
                "noise_node_chars": int(noise_node_chars),
            })

        self.given_projects = self._extract_given_projects(current_user_prompt)
        self._given_projects_open_idx = 0
        self._finish_block_counts = Counter()
        # reset failure-policy state
        self._blocked_queries = {}
        self._unproductive_queries = Counter()
        self._constraints_written = set()
        self._last_progress_step = 0
        self._last_exec_tool = None
        self._exec_tool_streak = 0
        if self.given_projects:
            self.mem.record_msg(
                "[SYSTEM] This task lists a specific set of projects. To avoid search loops, you can open each project's OFFICIAL PROFILE (typically docid D_TRUTH_####) and extract the required fields."
            )
            self._drain_mem_events(task_id=task_id, method=method, run_tag=run_tag, step=0)
        self._deadline_nudged = False
        self._last_search_query = None
        self._last_search_results = []
        self._last_search_repeat_streak = 0
        self._last_search_open_idx = 0
        self._recent_search_norms = deque(maxlen=max(4, int(self.cfg.search_cycle_window)))
        self._recent_search_newcand = deque(maxlen=max(4, int(self.cfg.search_cycle_window)))
        self._last_loop_escape_step = None
        self._adaptive_unfold_calls = 0
        self._forced_unfold_done = False
        self._last_finish_block_reason = None

        self._open_trace(run_tag=run_tag, method=method, task_id=task_id)

        system = self._build_system_prompt()
        base_messages: List[Dict[str, str]] = [
            {"role": "system", "content": system},
            {"role": "user", "content": current_user_prompt},
        ]

        def _inject_next_user_turn(reason: str, step_for_trace: int):
            nonlocal current_user_prompt
            if not pending_user_turns:
                return False
            nxt = pending_user_turns.pop(0)
            base_messages.append({"role": "user", "content": nxt})
            current_user_prompt = nxt
            self.counters["user_followups_injected"] += 1
            self.mem.record_summary(f"[USER_FOLLOWUP/{reason}] {nxt[:400]}", ttl=8)
            self._drain_mem_events(task_id=task_id, method=method, run_tag=run_tag, step=int(step_for_trace))
            try:
                self._trace({
                    "type": "user_turn_injected",
                    "task_id": task_id,
                    "method": method,
                    "run_tag": run_tag,
                    "step": int(step_for_trace),
                    "reason": str(reason),
                    "next_head": nxt[:200],
                    "pending_remaining": int(len(pending_user_turns)),
                })
            except Exception:
                pass
            return True

        def _inject_correction_user_turn(msg: str, reason: str, step_for_trace: int):
            nonlocal current_user_prompt
            if not msg:
                return False
            base_messages.append({"role": "user", "content": msg})
            current_user_prompt = msg
            self.mem.record_summary(f"[USER_CORRECTION/{reason}] {msg[:400]}", ttl=6)
            self._drain_mem_events(task_id=task_id, method=method, run_tag=run_tag, step=int(step_for_trace))
            try:
                self._trace({
                    "type": "user_turn_injected",
                    "task_id": task_id,
                    "method": method,
                    "run_tag": run_tag,
                    "step": int(step_for_trace),
                    "reason": str(reason),
                    "next_head": msg[:200],
                    "pending_remaining": int(len(pending_user_turns)),
                })
            except Exception:
                pass
            return True

        def _is_closed_book_now() -> bool:
            if not closed_book_final:
                return False
            up = (current_user_prompt or "")
            return ("[CLOSED-BOOK]" in up) or ("CLOSED BOOK" in up)

        start_time = time.time()

        try:
            for step in range(self.cfg.max_steps):
                remaining = self.cfg.max_steps - step
                if (not self._deadline_nudged) and remaining <= self.cfg.deadline_finish_nudge_steps:
                    self._deadline_nudged = True
                    if self.counters.get("open_page_calls", 0) >= self.cfg.min_open_pages_before_finish:
                        self.mem.record_msg(
                            "[SYSTEM] Deadline approaching. Stop searching/repeating. Use already opened evidence and CALL finish NOW. "
                            "finish.args.answer must be non-empty; finish.args.explanation must include 'Evidence docids: D_...'. "
                            "Example: {\"tool\":\"finish\",\"args\":{\"answer\":\"Project_0001 | City_02\",\"explanation\":\"Evidence docids: D_TRUTH_0001\"}}"
                        )
                    else:
                        self.mem.record_msg(
                            "[SYSTEM] Deadline approaching. You MUST open at least one page (open_page) and then CALL finish with docid evidence." 
                        )
                    self._trace({"type":"deadline_nudge","task_id":task_id,"method":method,"run_tag":run_tag,"step":step+1})

                # Multi-turn: optionally auto-inject follow-up user messages after the agent has done some work.
                if pending_user_turns and auto_inject_enabled:
                    oc = int(self.counters.get("open_page_calls", 0))
                    if (step >= int(mt_min_step)) and (oc >= int(mt_min_open_pages)):
                        _inject_next_user_turn("auto", step_for_trace=step)

                # Stage-aware unfold: on COMMIT (commit stages), proactively unfold around the
                # subtask question before the model emits committed titles. This counters folding
                # removing the exact TITLE string from ACTIVE_CONTEXT at commit time.
                try:
                    if (
                        str(method or '').lower().startswith('goc')
                        and self.cfg.stage_aware_unfold_on_commit
                        and ('/ COMMIT' in (current_user_prompt or ''))
                    ):
                        # Parse commit index from the prompt (supports both two-stage and multi-commit DAG tasks).
                        ci = 1
                        try:
                            m = re.search(r"\[SUBTASK\s+(\d+)\s*/\s*COMMIT\]", str(current_user_prompt or ''))
                            if m:
                                ci = int(m.group(1))
                            elif '[FOLLOW-UP 1' in (current_user_prompt or ''):
                                ci = 1
                        except Exception:
                            ci = 1

                        already = False
                        try:
                            already = (ci in getattr(self, '_stage_commit_unfold_done_for', set()))
                        except Exception:
                            already = False

                        if not already:
                            # Prefer extracting the explicit question line (Qk: ...).
                            q = ''
                            try:
                                for ln in str(current_user_prompt).splitlines():
                                    ln2 = ln.strip()
                                    if re.match(r"^Q\d+:", ln2):
                                        q = ln2.split(':', 1)[1].strip()
                                        break
                            except Exception:
                                q = ''
                            if not q and ci == 1:
                                q = (self._q1_text or '').strip()
                            query = q if q else (current_user_prompt or '')
                            if query:
                                query = query + ' | TITLE'
                                self._run_unfold(query, k=int(self.cfg.stage_commit_unfold_k), reason='stage_commit', step=step, task_id=task_id, method=method, run_tag=run_tag)
                                try:
                                    getattr(self, '_stage_commit_unfold_done_for').add(ci)
                                except Exception:
                                    pass
                                if ci == 1:
                                    self._stage_commit_unfold_done = True
                                self.counters['stage_commit_unfold_calls'] += 1
                                self._trace({
                                    'type': 'stage_commit_unfold',
                                    'task_id': task_id,
                                    'method': method,
                                    'run_tag': run_tag,
                                    'step': step,
                                    'commit_idx': ci,
                                    'query': query,
                                    'k': int(self.cfg.stage_commit_unfold_k),
                                    'have_q': bool(q),
                                })
                except Exception:
                    self.counters['stage_commit_unfold_errors'] += 1

                # Stage-aware unfold: on FINAL (often CLOSED-BOOK), proactively unfold around
                # committed anchors and/or Q1. This targets lost-in-the-middle failure modes
                # where the agent still opens the right pages but answers the wrong question.
                try:
                    if (
                        self.cfg.stage_aware_unfold_on_final
                        and (not self._stage_final_unfold_done)
                        and ("[FOLLOW-UP 2" in (current_user_prompt or "") or "/ FINAL" in (current_user_prompt or ""))
                    ):
                        q = (self._q1_text or "").strip()
                        # Multi-commit: use union of committed titles across commits to make unfold robust.
                        titles = []
                        try:
                            d = getattr(self, "_committed_supporting_titles_by_commit", None)
                            if isinstance(d, dict) and d:
                                seen = set()
                                for k in sorted(d.keys()):
                                    for t in d.get(k) or []:
                                        tt = str(t)
                                        if tt and tt not in seen:
                                            seen.add(tt)
                                            titles.append(tt)
                            else:
                                titles = self._committed_supporting_titles or []
                        except Exception:
                            titles = self._committed_supporting_titles or []
                        # Use a query that is robust across benchmarks: Q1 keywords + committed titles.
                        q_parts = []
                        if q:
                            q_parts.append(q)
                        if titles:
                            q_parts.append(" ".join(titles[:8]))
                        query = " | ".join([p for p in q_parts if p])
                        if query:
                            self._run_unfold(query, k=int(self.cfg.stage_final_unfold_k), reason='stage_final', step=step, task_id=task_id, method=method, run_tag=run_tag)
                            self._stage_final_unfold_done = True
                            self.counters["stage_final_unfold_calls"] += 1
                            self._trace({
                                "type": "stage_final_unfold",
                                "task_id": task_id,
                                "method": method,
                                "run_tag": run_tag,
                                "step": step,
                                "query": query,
                                "k": int(self.cfg.stage_final_unfold_k),
                                "have_committed_titles": bool(titles),
                                "have_q1": bool(q),
                            })
                except Exception:
                    self.counters["stage_final_unfold_errors"] += 1

                # GoC: proactively unfold relevant stored nodes when evidence coverage is low.
                # This happens *before* building the prompt so the recovered nodes can be used by the model.
                try:
                    self._maybe_adaptive_unfold(step, current_user_prompt, method=method, run_tag=run_tag, task_id=task_id)
                except Exception:
                    # Never let unfolding crash the run.
                    self.counters["adaptive_unfold_errors"] += 1

                # Late-binding helper (GoC): if the current user prompt asks for a deep field
                # (e.g., relocation_note) that was likely captured earlier but may have been folded,
                # proactively unfold by that keyword once per task.
                try:
                    if (not getattr(self, "_forced_unfold_done", False)) and (method == "GoC"):
                        up = (current_user_prompt or "").lower()
                        if ("relocation_note" in up) and (("follow-up" in up) or ("merge" in up) or ("late binding" in up)):
                            self.mem.unfold("relocation_note")
                            self._forced_unfold_done = True
                            self.counters["forced_unfold_calls"] += 1
                except Exception:
                    self.counters["forced_unfold_errors"] += 1
                # Stateless prompting: we DO NOT accumulate prior ACTIVE_CONTEXT in messages.
                ctx = self.mem.get_active_text()

                # What we actually send to the model.
                # By default (prompt_context_chars==0), include the full ACTIVE_CONTEXT as produced by the
                # memory manager (which already enforces budget_active in approx tokens).
                if int(getattr(self.cfg, "prompt_context_chars", 0) or 0) > 0:
                    ctx_for_prompt = ctx[-int(self.cfg.prompt_context_chars):]
                else:
                    ctx_for_prompt = ctx

                # What we log in traces (can be a shorter tail).
                ctx_for_log = ctx_for_prompt[-int(self.cfg.log_context_chars):] if int(self.cfg.log_context_chars) > 0 else ""

                messages = list(base_messages)
                if self.cfg.verbose:
                    # Always print a heartbeat per step (useful when running ALL methods).
                    atok = approx_token_count(ctx)
                    print(f"[{run_tag}][{method}][{task_id}] step={step+1}/{self.cfg.max_steps} active_tokâ‰ˆ{atok}", flush=True)

                if ctx_for_prompt:
                    messages.append({"role": "user", "content": "ACTIVE_CONTEXT:\n" + ctx_for_prompt})

                # GoC annotation hint (token-saving): inject a single compact line only on gated steps.
                goc_hint_sent_this_step = False
                if self._should_inject_goc_hint(step=step, remaining=remaining, current_user_prompt=current_user_prompt):
                    st = self._goc_stage_tag(current_user_prompt)
                    messages.append({"role": "user", "content": self._goc_hint_line(stage_tag=st)})
                    goc_hint_sent_this_step = True
                    if st == "stage1":
                        self._goc_stage1_hint_sent = True
                    elif st == "stage2":
                        self._goc_stage2_hint_sent = True
                    # doc_switch gate is a one-shot hint.
                    if self._goc_doc_switch_pending:
                        self._goc_doc_switch_pending = False

                # Prompt token estimate (heuristic) + per-task accumulation
                prompt_est = self._estimate_prompt_tokens(messages)
                self._accum_prompt_est(prompt_est)

                # Log prompt snapshot (optional)
                if self.cfg.log_messages:
                    self._trace({
                        "type": "prompt",
                        "task_id": task_id,
                        "method": method,
                        "run_tag": run_tag,
                        "step": step,
                        "messages": [
                            {"role": m["role"], "content": m["content"][: int(getattr(self.cfg, "log_message_chars", 6000) or 6000)]} for m in messages
                        ],
                        "active_tokens_est": approx_token_count(ctx),
                        "prompt_tokens_est": prompt_est,
                        "prompt_tokens_est_total": int(prompt_est.get("total", 0)),
                    })
                else:
                    self._trace({
                        "type": "prompt",
                        "task_id": task_id,
                        "method": method,
                        "run_tag": run_tag,
                        "step": step,
                        "active_tokens_est": approx_token_count(ctx),
                        "active_context_tail": (ctx_for_log[:2000] if ctx_for_log else ""),
                    })

                call, attempt_logs = self._call_model_for_json(messages, step=step)
                self._trace({
                    "type": "llm_attempts",
                    "task_id": task_id,
                    "method": method,
                    "run_tag": run_tag,
                    "step": step,
                    "attempts": attempt_logs,
                })

                if call is None:
                    if self.cfg.verbose:
                        print(f"[{run_tag}][{method}][{task_id}] step={step+1} JSON_PARSE_FAILED (continuing)")
                    continue

                # If we explicitly asked for GoC annotations on this step, but the model omitted them,
                # salvage by injecting an empty dict so downstream code paths stay well-typed.
                # (We still log a trace event so this is visible during debugging.)
                if goc_hint_sent_this_step and bool(getattr(self.cfg, "goc_annotation_force", True)):
                    if not isinstance(call.get("goc"), dict):
                        call["goc"] = {}
                        self.counters["goc_annotation_auto_added"] += 1
                        self._trace({
                            "type": "goc_annotation_missing",
                            "task_id": task_id,
                            "method": method,
                            "run_tag": run_tag,
                            "step": step,
                            "note": "model_output_missing_goc; injected empty dict",
                        })

                # Count proposed tool (from the model) separately from executed tool (after policy overrides).
                proposed_tool = call.get("tool")
                self.counters["tool_calls_proposed_total"] += 1
                if proposed_tool:
                    self.counters[f"tool_calls_proposed_{proposed_tool}"] += 1

                # Apply failure-policy constraints BEFORE executing the tool.
                call2, policy_evt = self._policy_preprocess_call(call, step=step)
                if policy_evt is not None:
                    self._trace({
                        "type": "policy_event",
                        "task_id": task_id,
                        "method": method,
                        "run_tag": run_tag,
                        "step": step,
                        **policy_evt,
                    })
                if call2 is None:
                    # Tool call blocked; consume the step but do not execute any tool.
                    if self.cfg.verbose:
                        print(f"[{run_tag}][{method}][{task_id}] POLICY_BLOCK proposed={proposed_tool}", flush=True)
                    continue

                tool = call2["tool"]
                args = call2.get("args", {}) or {}
                in_branch = (self.mem.current_thread != "main") if hasattr(self.mem, "current_thread") else False

                # Count executed tools
                self.counters["tool_calls_total"] += 1
                self.counters[f"tool_calls_{tool}"] += 1

                # For return-gating, count open_page TOOL calls in MAIN (includes cached views/docs).
                if tool == "open_page" and not in_branch:
                    self.counters["open_page_tool_calls_main"] += 1

                # Track executed-tool streak (for stuck-mode detection)
                if tool == self._last_exec_tool:
                    self._exec_tool_streak += 1
                else:
                    self._last_exec_tool = tool
                    self._exec_tool_streak = 1

                if self.cfg.verbose:
                    print(f"[{run_tag}][{method}][{task_id}] step={step+1}/{self.cfg.max_steps} tool={tool}")

                if tool == "search":

                    if _is_closed_book_now():
                        self.counters["closed_book_tool_blocked"] += 1
                        self.mem.record_msg("[SYSTEM] CLOSED-BOOK: search is disabled now. Use memory and call finish.")
                        self._trace({"type":"tool_blocked","task_id":task_id,"method":method,"run_tag":run_tag,"step":step,"tool":"search","reason":"closed_book"})
                        continue
                    query = args.get("query")
                    if not query:
                        self.counters["malformed_search_args"] += 1
                        self.mem.record_msg("[ASSISTANT] Malformed search call (missing query). Ignored.")
                        self._trace({
                            "type":"tool","task_id":task_id,"method":method,"run_tag":run_tag,"step":step,
                            "tool":"search","args":args,"malformed":True
                        })
                        continue
                    qnorm = self._normalize_query(query)
                    if qnorm and qnorm == self._last_search_query:
                        self._last_search_repeat_streak += 1
                    else:
                        self._last_search_query = qnorm
                        self._last_search_repeat_streak = 1
                        self._last_search_results = []
                        self._last_search_open_idx = 0

                    # If the model is repeating the exact same search, break loops by opening unseen evidence.
                    # v21: if the question provides an explicit project set, prefer opening those projects' truth docs.
                    if (
                        self.cfg.auto_open_on_repeat_search
                        and self._last_search_repeat_streak >= self.cfg.repeat_search_consecutive_threshold
                    ):
                        docid_to_open: Optional[str] = None

                        if self.cfg.open_given_projects_on_repeat_search and self.given_projects:
                            docid_to_open = self._next_unopened_given_docid()

                        if (docid_to_open is None) and self._last_search_results:
                            while self._last_search_open_idx < len(self._last_search_results):
                                cand = self._last_search_results[self._last_search_open_idx]
                                self._last_search_open_idx += 1
                                if cand not in self.opened_cache:
                                    docid_to_open = cand
                                    break

                        if docid_to_open:
                            self.counters["policy_overrides"] += 1

                            # Dedupe actual open calls
                            if self.cfg.open_page_dedupe and docid_to_open in self.opened_cache:
                                self.counters["open_page_cache_hits"] += 1
                                cached = self.opened_cache.get(docid_to_open, "")
                                prefix = self._authority_prefix(cached) if cached else ""
                                view = self._select_open_page_view(cached, {"section": "head"})
                                view_content = view.get("view") or ""
                                tag = "[POLICY OVERRIDE] " + (view.get("tag") or "")
                                obs = self._compose_open_page_observation_view(cached, view_content, prefix, tag)
                                # Cache the view so exact repeats are deduped.
                                view_key = self._open_view_key(docid_to_open, None, {"section": "head"})
                                self.opened_view_cache[view_key] = view_content
                                self._record_tool_step(
                                    step0=step,
                                    call=call,
                                    tool_name="open_page",
                                    args={"docid": docid_to_open, "section": "head"},
                                    observation=obs,
                                    docids=[docid_to_open, self._sig_open_docid(docid_to_open)],
                                    storage_text=cached,
                                    task_id=task_id,
                                    method=method,
                                    run_tag=run_tag,
                                )
                                self._drain_mem_events(task_id=task_id, method=method, run_tag=run_tag, step=step)
                                # Treat policy-driven evidence surfacing as progress to break stuck-mode heuristics.
                                self._last_progress_step = step
                                if docid_to_open not in self.evidence_docids:
                                    self.evidence_docids.append(docid_to_open)
                            else:
                                outp = self.tools.open_page(docid=docid_to_open)
                                self.counters["open_page_calls"] += 1
                                content = (outp.get("content") or "")
                                # Cache opened content losslessly (full) for later analysis/unfold.
                                self.opened_cache[outp["docid"]] = content
                                if outp["docid"] not in self.evidence_docids:
                                    self.evidence_docids.append(outp["docid"])
                                prefix = self._authority_prefix(content) if content else ""
                                view = self._select_open_page_view(content, {"section": "head"})
                                view_content = view.get("view") or ""
                                tag = "[POLICY OVERRIDE] " + (view.get("tag") or "")
                                obs = self._compose_open_page_observation_view(content, view_content, prefix, tag)
                                view_key = self._open_view_key(outp["docid"], None, {"section": "head"})
                                self.opened_view_cache[view_key] = view_content
                                self._record_tool_step(
                                    step0=step,
                                    call=call,
                                    tool_name="open_page",
                                    args={"docid": docid_to_open, "section": "head"},
                                    observation=obs,
                                    docids=[outp["docid"], self._sig_open_docid(outp["docid"])],
                                    storage_text=content,
                                    task_id=task_id,
                                    method=method,
                                    run_tag=run_tag,
                                )
                                self._drain_mem_events(task_id=task_id, method=method, run_tag=run_tag, step=step)
                                self._last_progress_step = step

                            self._trace({
                                "type": "policy_override",
                                "task_id": task_id,
                                "method": method,
                                "run_tag": run_tag,
                                "step": step,
                                "from_tool": "search",
                                "to_tool": "open_page",
                                "query": query,
                                "opened_docid": docid_to_open,
                            })
                            if self.cfg.verbose:
                                print(f"[{run_tag}][{method}][{task_id}] OVERRIDE search->open_page docid={docid_to_open}", flush=True)
                            continue

                    topk = int(args.get("topk", 10))
                    out = self.tools.search(query=query, topk=topk)

                    # Cache last search results for potential repeat-loop breaking
                    try:
                        docids = [x["docid"] for x in out]
                        self._last_search_results = docids
                        self._last_search_open_idx = 0
                    except Exception:
                        docids = []
                        self._last_search_results = []
                        self._last_search_open_idx = 0

                    self._record_search_query(query)
                    # Learn-from-failure: mark queries that yield no new evidence candidates
                    self._policy_post_search(query, docids, step=step)
                    self.counters["search_calls"] += 1

                    # Record into memory (active context)
                    # Provide titles/snippets in the observation to make tool output more usable
                    # and to help downstream matching for structured outputs (e.g., HotpotQA).
                    sig = self._sig_search_docid(query)
                    try:
                        lines: List[str] = []
                        # IMPORTANT: do NOT prefix each line with a bare integer like "1.".
                        # Some models mistakenly treat that index as the docid (e.g., open_page {docid:"5"}).
                        # Make the docid explicit.
                        for i, r in enumerate(out[: min(20, len(out))], start=1):
                            did = r.get("docid")
                            title = (r.get("title") or "").strip()
                            score = r.get("score")
                            snippet = (r.get("snippet") or "").strip().replace("\n", " ")
                            # Keep it compact; many corpora use long snippets.
                            if len(snippet) > 220:
                                snippet = snippet[:220] + "â€¦"
                            score_s = f"{float(score):.3f}" if isinstance(score, (int, float)) else ""
                            # Keep order implicit; the agent should copy-paste the docid string.
                            lines.append(f"docid={did} | title={title} | score={score_s} | {snippet}".rstrip())
                        observation = "\n".join(lines) if lines else "[]"
                    except Exception:
                        observation = str([x.get("docid") for x in out])

                    # Attach result docids as doc_refs so GoC can follow the evidence chain
                    # (search -> open_page -> finish) via dependency closure.
                    search_docids = [sig] + [d for d in (docids or []) if isinstance(d, str)]
                    self._record_tool_step(
                        step0=step,
                        call=call,
                        tool_name="search",
                        args=args,
                        observation=observation,
                        docids=search_docids,
                        task_id=task_id,
                        method=method,
                        run_tag=run_tag,
                    )
                    self._drain_mem_events(task_id=task_id, method=method, run_tag=run_tag, step=step)

                    self._trace({
                        "type": "tool",
                        "task_id": task_id,
                        "method": method,
                        "run_tag": run_tag,
                        "step": step,
                        "tool": "search",
                        "args": {"query": query, "topk": topk},
                        "result_docids": [x["docid"] for x in out[: min(20, len(out))]],
                    })

                elif tool == "open_page":

                    if _is_closed_book_now():
                        self.counters["closed_book_tool_blocked"] += 1
                        self.mem.record_msg("[SYSTEM] CLOSED-BOOK: open_page is disabled now. Use memory and call finish.")
                        self._trace({"type":"tool_blocked","task_id":task_id,"method":method,"run_tag":run_tag,"step":step,"tool":"open_page","reason":"closed_book"})
                        continue
                    docid = args.get("docid")
                    url = args.get("url")
                    if not docid and not url:
                        self.counters["malformed_open_page_args"] += 1
                        self.mem.record_msg("[ASSISTANT] Malformed open_page call (missing docid/url). Ignored.")
                        self._trace({
                            "type":"tool","task_id":task_id,"method":method,"run_tag":run_tag,"step":step,
                            "tool":"open_page","args":args,"malformed":True
                        })
                        continue

                    view_key = self._open_view_key(docid, url, args)

                    # Dedupe exact repeats of the *same view* (head/tail/find/offset).
                    if self.cfg.open_page_dedupe and view_key in self.opened_view_cache:
                        self.counters["open_page_cache_hits"] += 1
                        full_content = self.opened_cache.get(docid, "") if docid else ""
                        view_content = self.opened_view_cache.get(view_key, "")
                        # Include doc meta (docid/title/url) in the surfaced observation.
                        title = ""
                        try:
                            title = (self.tools.env.corpus.get(docid, {}).get("title") or "").strip() if docid else ""
                        except Exception:
                            title = ""
                        auth = self._authority_prefix(full_content) if full_content else ""
                        prefix_lines = []
                        if docid:
                            prefix_lines.append(f"DOCID: {docid}")
                        if title:
                            prefix_lines.append(f"TITLE: {title}")
                        if url:
                            prefix_lines.append(f"URL: {url}")
                        if auth:
                            prefix_lines.append(auth)
                        prefix = "\n".join(prefix_lines)
                        obs = self._compose_open_page_observation_view(full_content or view_content, view_content, prefix, "[CACHED VIEW]")

                        # Track doc switches for annotation gating.
                        try:
                            if self._goc_last_open_docid and docid and docid != self._goc_last_open_docid:
                                self._goc_doc_switch_pending = True
                                self.counters["goc_doc_switches"] += 1
                            if docid:
                                self._goc_last_open_docid = docid
                        except Exception:
                            pass

                        self._record_tool_step(
                            step0=step,
                            call=call,
                            tool_name="open_page",
                            args=args,
                            observation=obs,
                            docids=[docid, self._sig_open_docid(docid)] if docid else [],
                            storage_text=full_content or None,
                            task_id=task_id,
                            method=method,
                            run_tag=run_tag,
                        )
                        self._drain_mem_events(task_id=task_id, method=method, run_tag=run_tag, step=step)

                        if docid and docid not in self.evidence_docids:
                            self.evidence_docids.append(docid)

                        # Treat cached view surfacing as progress to avoid stuck-mode false positives.
                        self._last_progress_step = step
                        self._last_finish_block_reason = None

                        self._trace({
                            "type": "tool",
                            "task_id": task_id,
                            "method": method,
                            "run_tag": run_tag,
                            "step": step,
                            "tool": "open_page",
                            "args": {**args},
                            "opened_docid": docid,
                            "cached": True,
                            "view_key": view_key,
                            "content_preview": view_content[:800],
                        })
                        continue

                    # Fetch full content (lossless) either from cache or by calling env.
                    cached_doc = False
                    full_content = ""
                    if docid and docid in self.opened_cache:
                        cached_doc = True
                        self.counters["open_page_cache_hits"] += 1
                        full_content = self.opened_cache.get(docid, "")
                    else:
                        out = self.tools.open_page(docid=docid, url=url)
                        self.counters["open_page_calls"] += 1
                        full_content = (out.get("content") or "")
                        title = (out.get("title") or "")
                        # Canonicalize ids
                        docid = out.get("docid") or docid
                        url = out.get("url") or url
                        if docid:
                            self.opened_cache[docid] = full_content

                    # Title for cached docs (if not populated above)
                    if cached_doc:
                        try:
                            title = (self.tools.env.corpus.get(docid, {}).get("title") or "").strip() if docid else ""
                        except Exception:
                            title = ""

                    auth = self._authority_prefix(full_content)
                    prefix_lines = []
                    if docid:
                        prefix_lines.append(f"DOCID: {docid}")
                    if title:
                        prefix_lines.append(f"TITLE: {title.strip()}")
                    if url:
                        prefix_lines.append(f"URL: {url}")
                    if auth:
                        prefix_lines.append(auth)
                    prefix = "\n".join(prefix_lines)
                    view = self._select_open_page_view(full_content, args)
                    view_content = view.get("view") or ""
                    tag = view.get("tag") or ""

                    obs = self._compose_open_page_observation_view(full_content, view_content, prefix, tag)

                    # Track doc switches for annotation gating.
                    try:
                        if self._goc_last_open_docid and docid and docid != self._goc_last_open_docid:
                            self._goc_doc_switch_pending = True
                            self.counters["goc_doc_switches"] += 1
                        if docid:
                            self._goc_last_open_docid = docid
                    except Exception:
                        pass

                    # Store full content losslessly (for GoC), but keep a compact observation in ACTIVE_CONTEXT.
                    self._record_tool_step(
                        step0=step,
                        call=call,
                        tool_name="open_page",
                        args=args,
                        observation=obs,
                        docids=[docid, self._sig_open_docid(docid)] if docid else [],
                        storage_text=full_content,
                        task_id=task_id,
                        method=method,
                        run_tag=run_tag,
                    )
                    self._drain_mem_events(task_id=task_id, method=method, run_tag=run_tag, step=step)

                    # Cache per-view so exact repeats are deduped, but different views are allowed.
                    self.opened_view_cache[view_key] = view_content

                    # Progress: opened/surfaced evidence
                    self._last_progress_step = step
                    self._last_finish_block_reason = None

                    if docid and docid not in self.evidence_docids:
                        self.evidence_docids.append(docid)

                    self._trace({
                        "type": "tool",
                        "task_id": task_id,
                        "method": method,
                        "run_tag": run_tag,
                        "step": step,
                        "tool": "open_page",
                        "args": {**args, "docid": docid, "url": url},
                        "opened_docid": docid,
                        "cached": bool(cached_doc),
                        "view_key": view_key,
                        "view_tag": tag,
                        "content_preview": view_content[:800],
                    })




                elif tool == "branch":

                    if _is_closed_book_now():
                        self.counters["closed_book_tool_blocked"] += 1
                        self.mem.record_msg("[SYSTEM] CLOSED-BOOK: branch is disabled now. Use memory and call finish.")
                        self._trace({"type":"tool_blocked","task_id":task_id,"method":method,"run_tag":run_tag,"step":step,"tool":"branch","reason":"closed_book"})
                        continue
                    # Be robust to missing keys (models sometimes omit required fields).
                    desc = args.get("description")
                    prompt = args.get("prompt")
                    if in_branch:
                        self.counters["nested_branch_blocked"] += 1
                        self.mem.record_msg("[ASSISTANT] Nested branch disallowed; continue in current branch.")
                        blocked = True
                    else:
                        blocked = False
                        if not desc or not prompt:
                            self.counters["malformed_branch_args"] += 1
                            self.mem.record_msg("[ASSISTANT] Malformed branch call (missing description/prompt). Ignored.")
                        else:
                            self.mem.branch(description=desc, prompt=prompt)

                    self._trace({
                        "type": "tool",
                        "task_id": task_id,
                        "method": method,
                        "run_tag": run_tag,
                        "step": step,
                        "tool": "branch",
                        "args": args,
                        "blocked": bool(in_branch),
                        "malformed": bool((not in_branch) and (not desc or not prompt)),
                    })
                elif tool == "return":
                    injected = False
                    if in_branch:
                        msg = args.get("message")
                        if msg is None:
                            # Still exit the branch to avoid infinite return loops.
                            self.counters["malformed_return_args"] += 1
                            msg = ""
                        self.mem.return_from_branch(message=msg)
                    else:
                        # Multi-turn tasks: allow `return` in MAIN to request the next user follow-up.
                        if pending_user_turns:
                            # Return gating (benchmark-controlled): avoid requesting follow-ups too early.
                            # For commit-based benchmarks, we ONLY gate SUBTASK turns. MERGE turns are typically
                            # closed-book (tools disabled), so gating on open_page_calls would deadlock.
                            up_stage = (current_user_prompt or "")
                            stage_kind = "other"
                            try:
                                if ("[SUBTASK" in up_stage) and ("/ COMMIT" in up_stage):
                                    stage_kind = "subtask"
                                elif ("[MERGE" in up_stage) and ("/ COMMIT" in up_stage):
                                    stage_kind = "merge"
                                elif ("[FINAL" in up_stage) or ("[FOLLOW-UP" in up_stage) or ("closed-book final" in up_stage.lower()):
                                    stage_kind = "final"
                            except Exception:
                                stage_kind = "other"
                            oc_total = int(self.counters.get("open_page_tool_calls_main", 0))
                            oc_total_fetches = int(self.counters.get("open_page_calls", 0))
                            stage_steps = int(step)
                            oc = int(oc_total)
                            if bool(task_meta.get("return_gating_per_turn", False)):
                                last_step = int(self.counters.get("main_return_last_step", 0))
                                last_oc = int(self.counters.get("main_return_last_open_page_tool_calls", 0))
                                stage_steps = max(0, int(step) - int(last_step))
                                oc = max(0, int(oc_total) - int(last_oc))
                            if (stage_kind == "subtask") and ((stage_steps < int(rg_min_steps)) or (oc < int(rg_min_open_pages))):
                                self.counters["return_gated_blocked"] += 1
                                self.mem.record_msg(
                                    f"[SYSTEM] return blocked (gating/{stage_kind}): collect more evidence first. "
                                    f"Need stage_steps>={rg_min_steps} and stage_open_page_calls>={rg_min_open_pages}. "
                                    "Continue with search/open_page."
                                )
                                # Inject a corrective user message so the next prompt keeps only the active subtask.
                                try:
                                    _up = (current_user_prompt or "")
                                    _m = re.search(r"\[SUBTASK\s+(\d+)\s*/\s*COMMIT\]", _up)
                                    _expected = int(_m.group(1)) if _m else 1
                                    _ckey = ("subtask", "correction_return_gating", int(_expected))
                                    if _ckey not in self._correction_sent:
                                        _corr = (
                                            f"[SUBTASK {_expected} / COMMIT] Correction: "
                                            "CALL the `return` tool with args.message as DOUBLE-QUOTES JSON ONLY: "
                                            f"{{\"commit\":{_expected},\"a1\":\"...\",\"supporting_titles\":[\"Title A\",\"Title B\"]}}. "
                                            "supporting_titles MUST be exactly 2. a1 should be 1â€“2 sentences. "
                                            "Use tools to gather evidence, then return with the exact commit number."
                                        )
                                        _inject_correction_user_turn(_corr, reason="correction_return_gating", step_for_trace=step)
                                        self._correction_sent.add(_ckey)
                                except Exception:
                                    pass
                                self._trace({
                                    "type": "return_blocked",
                                    "stage_kind": stage_kind,
                                    "task_id": task_id,
                                    "method": method,
                                    "run_tag": run_tag,
                                    "step": step,
                                    "stage_steps": int(stage_steps),
                                    "open_page_calls_total": int(oc_total),
                                    "open_page_fetches_total": int(oc_total_fetches),
                                    "open_page_tool_calls_total": int(oc_total),
                                    "reason": "return_gating",
                                    "rg_min_steps": int(rg_min_steps),
                                    "rg_min_open_pages": int(rg_min_open_pages),
                                    "open_page_calls": oc,
                                    "open_page_tool_calls": oc,
                                })
                                continue
                            msg = args.get("message")
                            # Multi-commit schema validation: require well-formed commit/merge payloads before advancing turns.
                            mc_n = 0
                            try:
                                mc_n = int(task_meta.get("multi_commit_n", 0) or 0)
                            except Exception:
                                mc_n = 0
                            if mc_n >= 2:
                                import re as _re
                                def _parse_payload(x):
                                    """Best-effort parse of return.args.message into a dict (canonicalized)."""
                                    import json as _json
                                    import ast as _ast
                                    if isinstance(x, dict):
                                        return x
                                    if isinstance(x, str):
                                        t = (x or "").strip()
                                        if not t:
                                            return None
                                        # Strip common code fences
                                        t = _re.sub(r"^```(?:json)?\s*", "", t, flags=_re.I)
                                        t = _re.sub(r"\s*```\s*$", "", t)
                                        # Extract the first JSON object if extra text exists
                                        t2 = t
                                        if "{" in t and "}" in t:
                                            m = _re.search(r"\{[\s\S]*\}", t)
                                            if m:
                                                t2 = m.group(0)
                                        # JSON
                                        try:
                                            obj = _json.loads(t2)
                                            if isinstance(obj, dict):
                                                return obj
                                        except Exception:
                                            pass
                                        # Python-literal fallback
                                        try:
                                            obj = _ast.literal_eval(t2)
                                            if isinstance(obj, dict):
                                                return obj
                                        except Exception:
                                            pass
                                    return None

                                def _canonicalize_payload(d: dict) -> dict:
                                    out = dict(d)
                                    for k in ("commit", "merge", "winner_commit"):
                                        if k in out and isinstance(out[k], str):
                                            try:
                                                out[k] = int(out[k])
                                            except Exception:
                                                pass
                                    return out

                                def _schema_error_subtask(payload: dict, expected: int, n_titles_req: int) -> str:
                                    if not isinstance(payload, dict):
                                        return "parse_error"
                                    if "commit" not in payload:
                                        return "missing_field"
                                    try:
                                        if int(payload.get("commit", -1) or -1) != expected:
                                            return "wrong_commit"
                                    except Exception:
                                        return "wrong_commit"
                                    a1 = payload.get("a1")
                                    titles = payload.get("supporting_titles")
                                    if not (isinstance(a1, str) and a1.strip()):
                                        return "missing_field"
                                    if not (isinstance(titles, list)):
                                        return "missing_field"
                                    if len(titles) != n_titles_req:
                                        return "titles_len"
                                    if not all(isinstance(t, str) and t.strip() for t in titles):
                                        return "invalid_value"
                                    return "ok"

                                def _schema_error_merge(payload: dict, expected: int, mc_n: int) -> str:
                                    if not isinstance(payload, dict):
                                        return "parse_error"
                                    if "merge" not in payload:
                                        return "missing_field"
                                    try:
                                        if int(payload.get("merge", -1) or -1) != expected:
                                            return "wrong_commit"
                                    except Exception:
                                        return "wrong_commit"
                                    left = payload.get("left")
                                    right = payload.get("right")
                                    if not (isinstance(left, str) and left):
                                        return "missing_field"
                                    if not (isinstance(right, str) and right):
                                        return "missing_field"
                                    try:
                                        wc = int(payload.get("winner_commit", -1) or -1)
                                    except Exception:
                                        return "invalid_value"
                                    if wc < 1 or wc > mc_n:
                                        return "invalid_value"
                                    return "ok"
                                up_mc = (current_user_prompt or "")
                                msub = _re.search(r"\[SUBTASK\s+(\d+)\s*/\s*COMMIT\]", up_mc)
                                mmerge = _re.search(r"\[MERGE\s+(\d+)\s*/\s*COMMIT\]", up_mc)
                                if msub:
                                    expected = int(msub.group(1))
                                    payload = _parse_payload(msg)
                                    if isinstance(payload, dict):
                                        payload = _canonicalize_payload(payload)
                                    n_titles_req = int(task_meta.get("finish_supporting_titles_n", 2) or 2)
                                    titles = payload.get("supporting_titles") if isinstance(payload, dict) else None
                                    ok_fields = (isinstance(payload, dict)
                                                 and isinstance(payload.get("a1"), str) and payload.get("a1").strip()
                                                 and isinstance(titles, list) and len(titles) == n_titles_req
                                                 and all(isinstance(t, str) and t.strip() for t in titles))
                                    commit_val = None
                                    try:
                                        commit_val = int(payload.get("commit", -1) or -1) if isinstance(payload, dict) else None
                                    except Exception:
                                        commit_val = None
                                    ok = bool(ok_fields) and (commit_val == expected)
                                    schema_error_type = _schema_error_subtask(payload, expected, n_titles_req)

                                    # Optional auto-fix: accept when only commit mismatch is present.
                                    try:
                                        schema_autofix = bool(task_meta.get("schema_autofix_commit_mismatch", False))
                                    except Exception:
                                        schema_autofix = False
                                    if (not ok) and ok_fields and (schema_error_type == "wrong_commit") and schema_autofix:
                                        provided = commit_val
                                        try:
                                            payload["commit"] = int(expected)
                                        except Exception:
                                            payload["commit"] = expected
                                        ok = True
                                        self.counters["schema_autofix_commit_mismatch"] += 1
                                        try:
                                            self._trace({
                                                "type": "schema_autofix",
                                                "task_id": task_id,
                                                "method": method,
                                                "run_tag": run_tag,
                                                "step": step,
                                                "stage_kind": "subtask",
                                                "expected_commit": int(expected),
                                                "provided_commit": int(provided) if provided is not None else None,
                                            })
                                        except Exception:
                                            pass

                                    if not ok:
                                        self.counters["return_multicommit_schema_blocked"] += 1
                                        self.mem.record_msg(
                                            "[SYSTEM] return blocked: malformed SUBTASK commit payload. "
                                            "Call return with args.message={commit:i,a1:...,supporting_titles:[TitleA,TitleB]}."
                                        )
                                        try:
                                            _ckey = ("subtask", "correction_multicommit_schema", int(expected))
                                            if _ckey not in self._correction_sent:
                                                _corr = (
                                                    f"[SUBTASK {int(expected)} / COMMIT] Correction: "
                                                    "CALL the `return` tool with args.message as DOUBLE-QUOTES JSON ONLY: "
                                                    f"{{\"commit\":{int(expected)},\"a1\":\"...\",\"supporting_titles\":[\"Title A\",\"Title B\"]}}. "
                                                    "supporting_titles MUST be exactly 2. a1 should be 1â€“2 sentences. "
                                                    "Use the exact commit number shown in the tag."
                                                )
                                                _inject_correction_user_turn(_corr, reason="correction_multicommit_schema", step_for_trace=step)
                                                self._correction_sent.add(_ckey)
                                        except Exception:
                                            pass
                                        self._trace({
                                            "type": "return_blocked",
                                            "task_id": task_id,
                                            "method": method,
                                            "run_tag": run_tag,
                                            "step": step,
                                            "reason": "multicommit_schema",
                                            "stage": "subtask",
                                            "expected_commit": int(expected),
                                            "schema_error_type": schema_error_type,
                                            "payload_preview": (str(msg)[:200] if msg is not None else ""),
                                        })
                                        continue
                                elif mmerge:
                                    expected = int(mmerge.group(1))
                                    payload = _parse_payload(msg)
                                    if isinstance(payload, dict):
                                        payload = _canonicalize_payload(payload)
                                    ok = (isinstance(payload, dict)
                                          and int(payload.get("merge", -1) or -1) == expected
                                          and isinstance(payload.get("left"), str) and payload.get("left")
                                          and isinstance(payload.get("right"), str) and payload.get("right")
                                          and int(payload.get("winner_commit", -1) or -1) >= 1
                                          and int(payload.get("winner_commit", -1) or -1) <= mc_n)
                                    schema_error_type = _schema_error_merge(payload, expected, mc_n)
                                    if not ok:
                                        self.counters["return_multicommit_schema_blocked"] += 1
                                        self.mem.record_msg(
                                            "[SYSTEM] return blocked: malformed MERGE payload. "
                                            "Call return with args.message={merge:j,left:...,right:...,winner_commit:1..N}."
                                        )
                                        try:
                                            _ckey = ("merge", "correction_multicommit_schema", int(expected))
                                            if _ckey not in self._correction_sent:
                                                _corr = (
                                                    f"[MERGE {int(expected)} / COMMIT] Correction: "
                                                    "CALL the `return` tool with args.message as DOUBLE-QUOTES JSON ONLY: "
                                                    f"{{\"merge\":{int(expected)},\"left\":\"<left>\",\"right\":\"<right>\",\"winner_commit\":1..{int(mc_n)} }}. "
                                                    "a1 is not used here. Use the left/right identifiers from the MERGE prompt."
                                                )
                                                _inject_correction_user_turn(_corr, reason="correction_multicommit_schema", step_for_trace=step)
                                                self._correction_sent.add(_ckey)
                                        except Exception:
                                            pass
                                        self._trace({
                                            "type": "return_blocked",
                                            "task_id": task_id,
                                            "method": method,
                                            "run_tag": run_tag,
                                            "step": step,
                                            "reason": "multicommit_schema",
                                            "stage": "merge",
                                            "expected_merge": int(expected),
                                            "schema_error_type": schema_error_type,
                                            "payload_preview": (str(msg)[:200] if msg is not None else ""),
                                        })
                                        continue
                            # "return" can carry structured payloads (dict/list). Ensure we store a safe preview.
                            if msg is not None and msg != "":
                                try:
                                    import json as _json
                                    if isinstance(msg, (dict, list)):
                                        msg_s = _json.dumps(msg, ensure_ascii=False)
                                    else:
                                        msg_s = str(msg)
                                except Exception:
                                    msg_s = str(msg)
                                msg_s = msg_s[:800]
                                # Persist the agent's intermediate output (e.g., shortlist) so it can be reused.
                                self.mem.record_summary(f"[ASSISTANT_RETURN] {msg_s}", ttl=12)

                                # Two-stage COMMIT capture (HotpotQA/FEVER): if return.message contains
                                # supporting_titles (or equivalent), store them as a durable anchor and keep a copy in agent state.
                                # NOTE: models sometimes send this as a JSON *string*, not a dict. We parse both.
                                try:
                                    up = (current_user_prompt or "")
                                    is_commit_turn = ("[FOLLOW-UP 1" in up) or ("COMMIT" in up)
                                    if is_commit_turn and msg is not None:
                                        parsed = None

                                        def _parse_jsonish(s: str):
                                            """Parse a JSON-ish payload.

                                            Models frequently wrap JSON in code-fences, add leading/trailing text,
                                            or emit python-literal dicts. We try a few robust normalizations.
                                            """
                                            import json as _json
                                            import ast as _ast
                                            import re as _re

                                            t = (s or "").strip()
                                            if not t:
                                                return None

                                            # Strip common code fences.
                                            # ```json\n{...}\n``` or ```\n{...}\n```
                                            t = _re.sub(r"^```(?:json)?\s*", "", t, flags=_re.I)
                                            t = _re.sub(r"\s*```\s*$", "", t)

                                            # If there's surrounding text, try to extract the outermost {...} span.
                                            if "{" in t and "}" in t:
                                                i = t.find("{")
                                                j = t.rfind("}")
                                                if 0 <= i < j:
                                                    cand = t[i : j + 1].strip()
                                                else:
                                                    cand = t
                                            else:
                                                cand = t

                                            for attempt in [cand, t]:
                                                attempt = (attempt or "").strip()
                                                if not attempt:
                                                    continue
                                                # JSON first
                                                try:
                                                    obj = _json.loads(attempt)
                                                    if isinstance(obj, (dict, list)):
                                                        return obj
                                                except Exception:
                                                    pass
                                                # Python-literal fallback
                                                try:
                                                    obj = _ast.literal_eval(attempt)
                                                    if isinstance(obj, (dict, list)):
                                                        return obj
                                                except Exception:
                                                    pass
                                            return None

                                        if isinstance(msg, dict):
                                            parsed = msg
                                        elif isinstance(msg, str):
                                            parsed = _parse_jsonish(msg)

                                        # Determine commit index (two-stage defaults to 1).
                                        commit_idx = 1
                                        try:
                                            if isinstance(parsed, dict):
                                                for kk in ("commit", "commit_idx", "selected_commit", "winner_commit"):
                                                    if kk in parsed:
                                                        commit_idx = int(parsed.get(kk))
                                                        break
                                        except Exception:
                                            commit_idx = 1
                                        # Fallback: parse from the prompt marker.
                                        try:
                                            mci = re.search(r"\[SUBTASK\s+(\d+)\s*/\s*COMMIT\]", up)
                                            if mci:
                                                commit_idx = int(mci.group(1))
                                            elif "[FOLLOW-UP 1" in up:
                                                commit_idx = 1
                                        except Exception:
                                            pass

                                        titles = []
                                        if isinstance(parsed, dict):
                                            # Primary signal: explicit title lists
                                            for k in ["supporting_titles", "evidence_titles", "titles"]:
                                                raw_titles = parsed.get(k)
                                                if isinstance(raw_titles, list):
                                                    titles = [str(t).strip() for t in raw_titles if str(t).strip()]
                                                    if titles:
                                                        break
                                            # Common single-title keys
                                            if not titles:
                                                for k in ["evidence_title", "title"]:
                                                    v = parsed.get(k)
                                                    if isinstance(v, str) and v.strip():
                                                        titles = [v.strip()]
                                                        break
                                            # Trajectory checkpoints sometimes use primary/secondary title keys.
                                            if not titles and ("primary_title" in parsed):
                                                t1 = str(parsed.get("primary_title") or "").strip()
                                                t2 = str(parsed.get("secondary_title") or "").strip()
                                                titles = [t for t in [t1, t2] if t]

                                            # Keep only the first N to match typical HotpotQA expectations.
                                            n_keep = int(self.cfg.committed_supporting_titles_n or 2)
                                            titles_keep = titles[:n_keep]

                                            if titles_keep:
                                                # Store per-commit and 'last commit' views (backwards compatible).
                                                self._committed_supporting_titles = list(titles_keep)
                                                try:
                                                    self._committed_supporting_titles_by_commit[int(commit_idx)] = list(titles_keep)
                                                except Exception:
                                                    pass
                                                try:
                                                    av = parsed.get("a1")
                                                    if isinstance(av, str) and av.strip():
                                                        self._committed_answers_by_commit[int(commit_idx)] = av.strip()
                                                except Exception:
                                                    pass

                                                docids = [f"TITLE:{t}" for t in titles_keep]
                                                # Save commit anchor nid(s) for compact later references.
                                                try:
                                                    label = f"[COMMIT_SUPPORTING_TITLES c={int(commit_idx)}] " + ", ".join(titles_keep)
                                                    anchor_nid = self.mem.record_summary(label, docids=docids, ttl=None)
                                                    try:
                                                        self._goc_commit_anchor_nids_by_commit[int(commit_idx)] = anchor_nid
                                                    except Exception:
                                                        pass
                                                    # Also keep the legacy single-anchor pointer as 'last'.
                                                    self._goc_commit_anchor_nid = anchor_nid

                                                    # Add storage-only per-title anchors (do NOT add to ACTIVE_CONTEXT).
                                                    title_nids = {}
                                                    for ii, t in enumerate(titles_keep, start=1):
                                                        tnid = self.mem.add_node(
                                                            thread=getattr(self.mem, "current_thread", "MAIN"),
                                                            kind="summary",
                                                            text=f"[COMMIT_TITLE c={int(commit_idx)} i={ii}] {t}",
                                                            docids=[f"TITLE:{t}"],
                                                        )
                                                        title_nids[ii] = tnid
                                                        self.mem.add_edge("depends", tnid, anchor_nid)
                                                    try:
                                                        self._goc_commit_title_nids_by_commit[int(commit_idx)] = dict(title_nids)
                                                    except Exception:
                                                        pass
                                                    self._goc_commit_title_nids = dict(title_nids)
                                                except Exception:
                                                    self._goc_commit_anchor_nid = None

                                                self.counters["commit_titles_captured"] += 1

                                                try:
                                                    do_phase_fold = task_meta.get("goc_phase_end_fold", None)
                                                    if do_phase_fold is None:
                                                        pol = str(getattr(self.mem, "fold_policy", "") or "").lower().strip()
                                                        do_phase_fold = pol in {"dfs_doc", "doc_dfs", "dfs", "phase_end"}
                                                    if bool(do_phase_fold) and hasattr(self.mem, "phase_end_fold"):
                                                        self.mem.phase_end_fold(reason=f"commit_{int(commit_idx)}")
                                                        self._drain_mem_events(task_id=task_id, method=method, run_tag=run_tag, step=step)
                                                        self.counters["phase_end_folds"] += 1
                                                except Exception:
                                                    self.counters["phase_end_fold_errors"] += 1
                                except Exception:
                                    self.counters["commit_titles_capture_errors"] += 1

                            # Inject benchmark-controlled noise *between* stage-1 commit and final stage.
                            # This increases context pressure without extra LLM calls.
                            if (not self._noise_injected_after_stage1) and int(noise_nodes_after_stage1) > 0:
                                up = (current_user_prompt or "")
                                if ("[FOLLOW-UP 1" in up) or ("COMMIT" in up):
                                    self._noise_injected_after_stage1 = True
                                    _inject_noise_nodes("after_stage1", int(noise_nodes_after_stage1))

                            # Optional: inject noise after every commit (multi-commit DAG tasks).
                            try:
                                if int(noise_nodes_after_commit) > 0:
                                    upn = (current_user_prompt or '')
                                    ci = 1
                                    try:
                                        mci = re.search(r"\[SUBTASK\s+(\d+)\s*/\s*COMMIT\]", upn)
                                        if mci:
                                            ci = int(mci.group(1))
                                    except Exception:
                                        ci = 1
                                    if ci not in self._noise_injected_after_commit:
                                        self._noise_injected_after_commit.add(ci)
                                        _inject_noise_nodes(f"after_commit_{ci}", int(noise_nodes_after_commit))
                            except Exception:
                                self.counters["noise_after_commit_errors"] += 1

                            injected = _inject_next_user_turn("return", step_for_trace=step)
                            if injected and bool(task_meta.get("return_gating_per_turn", False)):
                                self.counters["main_return_last_step"] = int(step)
                                # Track open_page TOOL calls in MAIN for per-turn return gating.
                                self.counters["main_return_last_open_page_tool_calls"] = int(self.counters.get("open_page_tool_calls_main", 0))
                                # Keep legacy counter for backwards-compat/debugging (unique doc fetches).
                                self.counters["main_return_last_open_pages"] = int(self.counters.get("open_page_calls", 0))
                            if not injected:
                                self.counters["return_in_main_ignored"] += 1
                                self.mem.record_msg("[SYSTEM] No follow-up user turn is available. Continue with search/open_page/finish.")
                        else:
                            # Strong guidance to prevent repeated invalid returns.
                            self.counters["return_in_main_ignored"] += 1
                            self.mem.record_msg(
                                "[SYSTEM] You are in MAIN (no active branch). `return` is invalid here. "
                                "Next step MUST be one of: search, open_page, finish."
                            )
                    self._trace({
                        "type": "tool",
                        "task_id": task_id,
                        "method": method,
                        "run_tag": run_tag,
                        "step": step,
                        "tool": "return",
                        "args": args,
                        "ignored": bool((not in_branch) and (not injected)),
                        "injected": bool(injected),
                        "in_branch": bool(in_branch),
                    })

                elif tool == "finish":
                    # Finish gating: block premature finish (no evidence, too early, missing docids).
                    open_calls = int(self.counters.get("open_page_calls", 0))

                    # Multi-turn tasks: do not allow finishing before follow-up turns are delivered.
                    if pending_user_turns:
                        self.counters["premature_finish_blocked"] += 1
                        self._last_finish_block_reason = "pending_user_turns"
                        _fb_msg = "[SYSTEM] finish blocked: there is a follow-up user message pending. Call `return` to receive it."
                        if (not commit_flow) and bool(auto_inject_enabled):
                            _fb_msg = _fb_msg + " (or continue until it is auto-injected)."
                        self.mem.record_msg(_fb_msg)
                        self._trace({"type": "finish_blocked", "task_id": task_id, "method": method, "run_tag": run_tag, "step": step, "reason": "pending_user_turns"})
                        if self.cfg.verbose:
                            print(f"[{run_tag}][{method}][{task_id}] BLOCK_FINISH reason=pending_user_turns", flush=True)
                        continue

                    if (step + 1) < int(self.cfg.min_steps_before_finish):
                        self.counters["premature_finish_blocked"] += 1
                        self._last_finish_block_reason = "min_steps"
                        self.mem.record_msg(
                            f"[SYSTEM] finish blocked: need at least {self.cfg.min_steps_before_finish} steps. "
                            "Next call should be search/open_page."
                        )
                        self._trace({"type": "finish_blocked", "task_id": task_id, "method": method, "run_tag": run_tag, "step": step, "reason": "min_steps"})
                        if self.cfg.verbose:
                            print(f"[{run_tag}][{method}][{task_id}] BLOCK_FINISH reason=min_steps", flush=True)
                        continue

                    if open_calls < int(self.cfg.min_open_pages_before_finish):
                        self.counters["premature_finish_blocked"] += 1
                        self._last_finish_block_reason = "no_open_page"
                        self.mem.record_msg(
                            f"[SYSTEM] finish blocked: need at least {self.cfg.min_open_pages_before_finish} open_page calls for evidence. "
                            "Next call MUST be search -> open_page."
                        )
                        self._trace({"type": "finish_blocked", "task_id": task_id, "method": method, "run_tag": run_tag, "step": step, "reason": "no_open_page"})
                        if self.cfg.verbose:
                            print(f"[{run_tag}][{method}][{task_id}] BLOCK_FINISH reason=no_open_page", flush=True)
                        continue

                    ans_raw = args.get("answer")
                    if ans_raw is None:
                        # common alternative keys from LLMs
                        ans_raw = args.get("final") or args.get("final_answer") or args.get("result") or args.get("output")
                    # IMPORTANT: keep structured answers JSON-parseable.
                    # Some benchmarks (e.g., HotpotQA) expect finish.args.answer to be a JSON object
                    # serialized as a string. Using str(dict) produces single-quotes and breaks parsing.
                    ans = ""
                    if ans_raw is not None:
                        try:
                            import json as _json
                            import ast as _ast

                            if isinstance(ans_raw, (dict, list)):
                                ans = _json.dumps(ans_raw, ensure_ascii=False)
                            elif isinstance(ans_raw, str):
                                s = ans_raw.strip()
                                # If the model emitted a python-literal dict, normalize it to JSON.
                                if s.startswith("{") and "'" in s and '"' not in s:
                                    try:
                                        obj = _ast.literal_eval(s)
                                        if isinstance(obj, (dict, list)):
                                            s = _json.dumps(obj, ensure_ascii=False)
                                    except Exception:
                                        pass
                                ans = s
                            else:
                                ans = str(ans_raw).strip()
                        except Exception:
                            ans = str(ans_raw).strip()
                    expl0 = (args.get("explanation") or "")

                    # Normalize: ensure the final answer is always stored in args["answer"] for downstream evaluation.
                    if ans:
                        args["answer"] = ans

                    # HotpotQA (and similar) contract: finish.args.answer must be a JSON object serialized as a string
                    # with keys {a1, supporting_titles}. Many failures in long-horizon settings come from the model
                    # emitting plain text here; that confounds memory-method comparisons. We salvage format without
                    # extra LLM calls whenever possible (using the committed titles when available).
                    try:
                        finish_fmt = str(task_meta.get("finish_answer_format", "") or "").lower()
                        if finish_fmt == "hotpotqa_json" and ans:
                            import json as _json

                            gold = task_meta.get("gold") or {}
                            gold_a1 = str(gold.get("a1", "") or "").strip().lower()
                            is_yesno = gold_a1 in {"yes", "no"}
                            n_titles = int(task_meta.get("finish_supporting_titles_n", 2) or 2)
                            committed = list(self._committed_supporting_titles or [])

                            def _extract_yesno(text: str) -> str:
                                m = re.search(r"\b(yes|no)\b", (text or "").lower())
                                return m.group(1) if m else ""

                            def _extract_a1_from_text(text: str) -> str:
                                s = (text or "").strip()
                                # Strip common trailing evidence sections.
                                s = re.split(r"(?i)\bevidence\b|\bdocids?\b", s, maxsplit=1)[0].strip()
                                # Remove a leading label.
                                s = re.sub(r"(?is)^\s*(final\s+answer|answer)\s*[:\-]\s*", "", s).strip()
                                # Take first non-empty line / sentence.
                                line = next((ln.strip() for ln in s.splitlines() if ln.strip()), s)
                                # Keep it short-ish; evaluator uses F1 so extra prose hurts.
                                if len(line) > 240:
                                    line = line[:240].rsplit(" ", 1)[0]
                                return line.strip()

                            def _normalize_a1(a1: str, raw_text: str) -> str:
                                if is_yesno:
                                    y = _extract_yesno(a1) or _extract_yesno(raw_text)
                                    return y if y in {"yes", "no"} else (a1 or "").strip().lower()
                                return (a1 or "").strip()

                            obj = None
                            try:
                                obj = _json.loads(ans)
                            except Exception:
                                obj = None

                            if isinstance(obj, dict):
                                # Map common alternative keys.
                                if "a1" not in obj:
                                    for k in ("answer", "final", "final_answer", "result", "output", "a"):
                                        if k in obj:
                                            obj["a1"] = obj.get(k)
                                            break
                                if "supporting_titles" not in obj or not isinstance(obj.get("supporting_titles"), list):
                                    if committed:
                                        obj["supporting_titles"] = committed[:n_titles]
                                    else:
                                        obj["supporting_titles"] = []
                                # Enforce list length.
                                if isinstance(obj.get("supporting_titles"), list):
                                    obj["supporting_titles"] = list(obj["supporting_titles"])[:n_titles]
                                obj["a1"] = _normalize_a1(str(obj.get("a1", "") or ""), ans)
                                ans = _json.dumps(obj, ensure_ascii=False)
                                args["answer"] = ans
                            else:
                                # Not JSON: salvage into the required JSON shape if we have a committed title anchor.
                                if committed:
                                    a1 = _normalize_a1(_extract_a1_from_text(ans), ans)
                                    obj2 = {"a1": a1, "supporting_titles": committed[:n_titles]}
                                    ans = _json.dumps(obj2, ensure_ascii=False)
                                    args["answer"] = ans
                                    self.counters["finish_hotpot_json_salvaged"] += 1
                                    self._trace({
                                        "type": "finish_json_salvaged",
                                        "task_id": task_id,
                                        "method": method,
                                        "run_tag": run_tag,
                                        "step": step,
                                        "supporting_titles": committed[:n_titles],
                                    })
                                else:
                                    # If we cannot salvage (no anchor), block and reprompt.
                                    self.counters["premature_finish_blocked"] += 1
                                    self._last_finish_block_reason = "finish_answer_schema"
                                    self.mem.record_msg(
                                        "[SYSTEM] finish blocked: for HotpotQA, finish.args.answer MUST be JSON with keys {a1, supporting_titles} and no extra text. "
                                        "Example: {\"a1\":\"yes\",\"supporting_titles\":[\"Title1\",\"Title2\"]}."
                                    )
                                    self._trace({"type": "finish_blocked", "task_id": task_id, "method": method, "run_tag": run_tag, "step": step, "reason": "finish_answer_schema"})
                                    continue

                        # FEVER prepared (and similar) contract: JSON {label, evidence_titles}
                        # We salvage schema without extra LLM calls to avoid format confounds in long-horizon setups.
                        if finish_fmt == "fever_json" and ans:
                            import json as _json

                            committed = list(self._committed_supporting_titles or [])

                            def _norm_fever_label(text: str) -> str:
                                t = (text or "").strip().lower()
                                if "support" in t:
                                    return "supports"
                                if "refute" in t:
                                    return "refutes"
                                if "not enough" in t or "nei" in t or "unknown" in t or "insufficient" in t:
                                    return "not_enough_info"
                                # If already canonical
                                if t in {"supports", "refutes", "not_enough_info"}:
                                    return t
                                return (t or "").strip()

                            obj = None
                            try:
                                obj = _json.loads(ans)
                            except Exception:
                                obj = None

                            if isinstance(obj, dict):
                                # Map label keys
                                if "label" not in obj:
                                    for k in ("verdict", "classification", "result"):
                                        if k in obj:
                                            obj["label"] = obj.get(k)
                                            break
                                obj["label"] = _norm_fever_label(str(obj.get("label") or ""))

                                # Map titles keys
                                if "evidence_titles" not in obj or not isinstance(obj.get("evidence_titles"), list):
                                    for k in ("supporting_titles", "titles"):
                                        v = obj.get(k)
                                        if isinstance(v, list):
                                            obj["evidence_titles"] = v
                                            break
                                if "evidence_titles" not in obj or not isinstance(obj.get("evidence_titles"), list):
                                    obj["evidence_titles"] = committed if committed else []

                                # Light normalize list
                                obj["evidence_titles"] = [str(t).strip() for t in (obj.get("evidence_titles") or []) if str(t).strip()]
                                ans = _json.dumps(obj, ensure_ascii=False)
                                args["answer"] = ans
                            else:
                                # Not JSON: attempt a lightweight salvage. Use committed titles if available.
                                lab = _norm_fever_label(ans)
                                obj2 = {
                                    "label": lab if lab in {"supports", "refutes", "not_enough_info"} else "",
                                    "evidence_titles": committed if committed else [],
                                }
                                ans = _json.dumps(obj2, ensure_ascii=False)
                                args["answer"] = ans
                                self.counters["finish_fever_json_salvaged"] += 1

                        # Lost-in-the-Middle contract: JSON {a1, a2} (a2 is title or evidence sentence)
                        if finish_fmt in {"litm_json_title", "litm_json_sentence"} and ans:
                            import json as _json
                            committed = list(self._committed_supporting_titles or [])

                            def _extract_a1_from_text(text: str) -> str:
                                s = (text or "").strip()
                                s = re.split(r"(?i)\bevidence\b|\bdocids?\b", s, maxsplit=1)[0].strip()
                                s = re.sub(r"(?is)^\s*(final\s+answer|answer)\s*[:\-]\s*", "", s).strip()
                                line = next((ln.strip() for ln in s.splitlines() if ln.strip()), s)
                                if len(line) > 240:
                                    line = line[:240].rsplit(" ", 1)[0]
                                return line.strip()

                            obj = None
                            try:
                                obj = _json.loads(ans)
                            except Exception:
                                obj = None

                            if isinstance(obj, dict):
                                if "a1" not in obj:
                                    for k in ("answer", "final", "final_answer", "result", "output", "a"):
                                        if k in obj:
                                            obj["a1"] = obj.get(k)
                                            break
                                if "a2" not in obj:
                                    for k in ("title", "evidence", "evidence_title"):
                                        if k in obj:
                                            obj["a2"] = obj.get(k)
                                            break
                                # Title variant: if missing, use committed title anchor.
                                if finish_fmt == "litm_json_title" and (not str(obj.get("a2") or "").strip()) and committed:
                                    obj["a2"] = committed[0]
                                obj["a1"] = str(obj.get("a1") or "").strip()
                                obj["a2"] = str(obj.get("a2") or "").strip()
                                ans = _json.dumps(obj, ensure_ascii=False)
                                args["answer"] = ans
                            else:
                                # Non-JSON: salvage only when we can anchor a2 (title).
                                if finish_fmt == "litm_json_title" and committed:
                                    obj2 = {"a1": _extract_a1_from_text(ans), "a2": committed[0]}
                                    ans = _json.dumps(obj2, ensure_ascii=False)
                                    args["answer"] = ans
                                    self.counters["finish_litm_json_salvaged"] += 1
                    except Exception:
                        self.counters["finish_hotpot_json_salvage_errors"] += 1

                    # Optional: enforce committed supporting_titles to avoid drift confounds.
                    # This is benchmark-agnostic but only activates when:
                    #   (1) the agent previously captured a commit list, and
                    #   (2) the final answer is JSON-like and contains (or should contain) supporting_titles.
                    try:
                        scope = str(getattr(self.cfg, "enforce_committed_supporting_titles", "none") or "none").lower()
                        should_enforce = (
                            scope == "all" or
                            (scope == "goc_only" and str(method) == "GoC")
                        )
                        # If multi-commit DAG bench provides selected_commit/winner_commit,
                        # enforce the titles for that specific commit; otherwise fall back to last-commit.
                        committed = self._committed_supporting_titles or []
                        try:
                            d = getattr(self, "_committed_supporting_titles_by_commit", None)
                            if isinstance(d, dict) and d:
                                # Delay commit selection until we parse the final JSON object below.
                                pass
                        except Exception:
                            pass
                        finish_fmt2 = str(task_meta.get("finish_answer_format", "") or "").lower()
                        if should_enforce and committed and ans:
                            import json as _json
                            obj = None
                            try:
                                obj = _json.loads(ans)
                            except Exception:
                                obj = None
                            if isinstance(obj, dict):
                                # Pick commit-specific committed titles if the final output declares it.
                                try:
                                    d = getattr(self, "_committed_supporting_titles_by_commit", None)
                                    if isinstance(d, dict) and d:
                                        csel = None
                                        for k in ("selected_commit", "winner_commit", "commit", "commit_idx"):
                                            if k in obj:
                                                try:
                                                    csel = int(obj.get(k))
                                                except Exception:
                                                    csel = None
                                                break
                                        if csel is not None and csel in d:
                                            committed = list(d.get(csel) or committed)
                                except Exception:
                                    pass
                                # HotpotQA-like
                                if ("supporting_titles" in obj) or (finish_fmt2 == "hotpotqa_json"):
                                    obj["supporting_titles"] = list(committed)

                                # FEVER-like
                                if ("evidence_titles" in obj) or (finish_fmt2 == "fever_json"):
                                    obj["evidence_titles"] = list(committed)

                                # LITM title two-stage: enforce a2 title
                                if finish_fmt2 == "litm_json_title":
                                    obj["a2"] = str(committed[0])
                                args["answer"] = _json.dumps(obj, ensure_ascii=False)
                                ans = args["answer"]
                                self.counters["finish_enforced_committed_titles"] += 1
                                self._trace({
                                    "type": "enforce_committed_titles",
                                    "task_id": task_id,
                                    "method": method,
                                    "run_tag": run_tag,
                                    "step": step,
                                    "committed": committed,
                                })
                    except Exception:
                        self.counters["finish_enforce_committed_titles_errors"] += 1

                    # If answer is missing/empty, try to salvage from explanation (models often put the answer there).
                    if not ans:
                        cand: Optional[str] = None
                        expl_s = (expl0 or "").strip()
                        if expl_s:
                            # Strong pattern: '<Project_####> | <City_..>' (or similar).
                            m_pair = re.search(r"\b(Project_\d{4})\b\s*\|\s*\b([A-Za-z]+_\d+)\b", expl_s)
                            if m_pair:
                                cand = f"{m_pair.group(1)} | {m_pair.group(2)}"
                            else:
                                # Common phrasing: "Project_0041 ... headquarters is City_42"
                                m_hq = re.search(r"\b(Project_\d{4})\b.*?\bheadquarters\b.*?\b([A-Za-z]+_\d+)\b", expl_s, flags=re.I|re.S)
                                if m_hq:
                                    cand = f"{m_hq.group(1)} | {m_hq.group(2)}"
                                else:
                                    # Explicit "Answer: ..." line
                                    m_ans = re.search(r"(?is)\banswer\s*[:\-]\s*(.+?)(?:\n|$)", expl_s)
                                    if m_ans:
                                        cand = m_ans.group(1).strip()
                                    else:
                                        # Heuristic: take the first non-empty line before an Evidence/Docids section.
                                        head = re.split(r"(?i)\bevidence\b|\bdocids?\b", expl_s, maxsplit=1)[0].strip()
                                        if head:
                                            cand = next((ln.strip() for ln in head.splitlines() if ln.strip()), None)

                        if cand:
                            ans = str(cand).strip()
                            args["answer"] = ans
                            self.counters["finish_answer_salvaged"] += 1

                    if not ans:
                        self.counters["premature_finish_blocked"] += 1
                        self._last_finish_block_reason = "empty_answer"
                        self.mem.record_msg("[SYSTEM] finish blocked: empty answer. Put the final answer into finish.args.answer (a non-empty short string).")
                        self.mem.record_msg('[SYSTEM] Example: {"tool":"finish","args":{"answer":"<SHORT ANSWER>","explanation":"Evidence docids: D_TRUTH_0001"}}')
                        self._trace({"type": "finish_blocked", "task_id": task_id, "method": method, "run_tag": run_tag, "step": step, "reason": "empty_answer"})
                        if self.cfg.verbose:
                            print(f"[{run_tag}][{method}][{task_id}] BLOCK_FINISH reason=empty_answer", flush=True)
                        continue
                    # v21: finish validator - enforce answer uses one of the GIVEN projects when the question provides an explicit project set.
                    if self.cfg.validate_answer_in_given_projects and self.given_projects:
                        allowed = set(self.given_projects)
                        # Expected format: '<ProjectName> | <Headquarters>'
                        proj_text = ans.split('|', 1)[0].strip() if '|' in ans else ans.strip()
                        m_proj = re.search(r"\bProject_\d{4}\b", proj_text)
                        proj = m_proj.group(0) if m_proj else ""
                        if (not proj) or (proj not in allowed):
                            self.counters["finish_invalid_project_blocked"] += 1
                            self._last_finish_block_reason = "invalid_project"
                            if self._should_block_finish("invalid_project"):
                                self.mem.record_msg(
                                    "[SYSTEM] finish blocked: answer project must be one of the GIVEN projects from the question. "
                                    "Re-check evidence and answer format '<Project_####> | <Headquarters>'."
                                )
                                self.mem.record_msg("[SYSTEM] Allowed projects: " + ", ".join(self.given_projects))
                                self._trace({
                                    "type": "finish_blocked",
                                    "task_id": task_id,
                                    "method": method,
                                    "run_tag": run_tag,
                                    "step": step,
                                    "reason": "invalid_project",
                                    "answer": ans,
                                })
                                if self.cfg.verbose:
                                    print(f"[{run_tag}][{method}][{task_id}] BLOCK_FINISH reason=invalid_project", flush=True)
                                continue


                    if self.cfg.require_docids_in_finish:
                        # Require at least one docid-like token in explanation (e.g., D_TRUTH_0001).
                        # Instead of blocking indefinitely, auto-append evidence docids from opened pages.
                        if self.cfg.require_docids_in_finish:
                            # Require at least one VALID opened docid cited in explanation (e.g., D_TRUTH_0001).
                            # If the model cites a hallucinated docid (e.g., D_PROJECT_0001), treat as missing and auto-append evidence.
                            cited = re.findall(r"\bD_[A-Z0-9_\-]+\b", expl0 or "")
                            valid = set(self.evidence_docids) | set(self.opened_cache.keys())
                            has_valid = any(d in valid for d in cited)
                            if not has_valid:
                                if self.evidence_docids:
                                    tail = ", ".join(self.evidence_docids[-3:])
                                    expl0 = (expl0 + "\n" if expl0 else "") + f"Evidence docids: {tail}"
                                    args["explanation"] = expl0
                                    self.counters["finish_docids_auto_appended"] += 1
                                else:
                                    # Fallback: block if we truly have no evidence docids recorded.
                                    self.counters["premature_finish_blocked"] += 1
                                    self._last_finish_block_reason = "missing_docids_no_evidence"
                                    self.mem.record_msg("[SYSTEM] finish blocked: explanation missing evidence docids. Cite docids from open_page.")
                                    self.mem.record_msg("[SYSTEM] You attempted to finish without evidence docids, and no opened docids were recorded. Call open_page first.")
                                    self._trace({"type": "finish_blocked", "task_id": task_id, "method": method, "run_tag": run_tag, "step": step, "reason": "missing_docids_no_evidence"})
                                    if self.cfg.verbose:
                                        print(f"[{run_tag}][{method}][{task_id}] BLOCK_FINISH reason=missing_docids_no_evidence", flush=True)
                                    continue

                    top_rep = self.search_query_counts.most_common(5)
                    elapsed = time.time() - start_time

                    # Drain memory events that may have been buffered before finishing
                    self._drain_mem_events(task_id, method, run_tag, step)

                    result = {
                        "answer": args.get("answer", ""),
                        "explanation": args.get("explanation", ""),
                        "confidence": args.get("confidence", ""),
                        "active_context": self.mem.get_active_text(),
                        "usage": dict(self.usage_accum),
                        "prompt_est_accum": {
                            "total": int(self.counters.get("prompt_est_total", 0)),
                            "system": int(self.counters.get("prompt_est_system", 0)),
                            "user_base": int(self.counters.get("prompt_est_user_base", 0)),
                            "followups": int(self.counters.get("prompt_est_followups", 0)),
                            "active_context_total": int(self.counters.get("prompt_est_active_context_total", 0)),
                            "active_tool_lines": int(self.counters.get("prompt_est_active_tool_lines", 0)),
                            "active_obs_lines": int(self.counters.get("prompt_est_active_obs_lines", 0)),
                            "active_noise_lines": int(self.counters.get("prompt_est_active_noise_lines", 0)),
                            "active_summary_lines": int(self.counters.get("prompt_est_active_summary_lines", 0)),
                            "active_meta_lines": int(self.counters.get("prompt_est_active_meta_lines", 0)),
                            "active_other_lines": int(self.counters.get("prompt_est_active_other_lines", 0)),
                        },
                        "mem_event_stats": {
                            "fold_events": int(self.counters.get("mem_fold_events", 0)),
                            "fold_removed_tokens_est": int(self.counters.get("mem_fold_removed_tokens_est", 0)),
                            "fold_overflow_tokens_est": int(self.counters.get("mem_fold_overflow_tokens_est", 0)),
                            "unfold_events": int(self.counters.get("mem_unfold_events", 0)),
                            "unfold_added_tokens_est": int(self.counters.get("mem_unfold_added_tokens_est", 0)),
                            "rag_unfold_events": int(self.counters.get("mem_rag_unfold_events", 0)),
                            "pef_fold_events": int(self.counters.get("mem_pef_fold_events", 0)),
                            "pef_roll_fold_events": int(self.counters.get("mem_pef_roll_fold_events", 0)),
                        },
                        "steps": step + 1,
                        "elapsed_sec": elapsed,
                        "tool_stats": {
                            "tool_calls_proposed_total": int(self.counters.get("tool_calls_proposed_total", 0)),
                            "tool_calls_total": int(self.counters["tool_calls_total"]),
                            "search_calls": int(self.counters["search_calls"]),
                            "open_page_calls": int(self.counters["open_page_calls"]),
                            "open_page_cache_hits": int(self.counters.get("open_page_cache_hits", 0)),
                            "policy_overrides": int(self.counters.get("policy_overrides", 0)),
                            "constraints_written": int(self.counters.get("constraints_written", 0)),
                            "constraints_write_errors": int(self.counters.get("constraints_write_errors", 0)),
                            "duplicate_open_blocked": int(self.counters.get("duplicate_open_blocked", 0)),
                            "blocked_search_query": int(self.counters.get("blocked_search_query", 0)),
                            "unproductive_searches": int(self.counters.get("unproductive_searches", 0)),
                            "search_queries_cooled_down": int(self.counters.get("search_queries_cooled_down", 0)),
                            "candidate_first_overrides": int(self.counters.get("candidate_first_overrides", 0)),
                            "search_cycle_break_overrides": int(self.counters.get("search_cycle_break_overrides", 0)),
                            "adaptive_unfold_calls": int(self.counters.get("adaptive_unfold_calls", 0)),
                            "adaptive_unfold_activated_nodes": int(self.counters.get("adaptive_unfold_activated_nodes", 0)),
                            "adaptive_unfold_errors": int(self.counters.get("adaptive_unfold_errors", 0)),
                            "mode_switch_overrides": int(self.counters.get("mode_switch_overrides", 0)),
                            "return_calls": int(self.counters.get("tool_calls_return", 0)),
                            "return_in_main_ignored": int(self.counters.get("return_in_main_ignored", 0)),
                            "malformed_branch_args": int(self.counters.get("malformed_branch_args", 0)),
                            "malformed_return_args": int(self.counters.get("malformed_return_args", 0)),
                            "malformed_search_args": int(self.counters.get("malformed_search_args", 0)),
                            "malformed_open_page_args": int(self.counters.get("malformed_open_page_args", 0)),
                            "repeated_search_count": int(self.counters["repeated_search_count"]),
                            "unique_search_queries": int(len(self.search_query_counts)),
                            "top_repeated_queries": [{"query": q, "count": c} for q, c in top_rep if c >= 2],
                            "json_parse_failures": int(self.counters["json_parse_failures"]),
                            "json_recoveries": int(self.counters["json_recoveries"]),
                            "finish_answer_salvaged": int(self.counters.get("finish_answer_salvaged", 0)),
                            "finish_hotpot_json_salvaged": int(self.counters.get("finish_hotpot_json_salvaged", 0)),
                            "finish_hotpot_json_salvage_errors": int(self.counters.get("finish_hotpot_json_salvage_errors", 0)),
                            "finish_docids_auto_appended": int(self.counters.get("finish_docids_auto_appended", 0)),
                            "finish_invalid_project_blocked": int(self.counters.get("finish_invalid_project_blocked", 0)),
                            "premature_finish_blocked": int(self.counters.get("premature_finish_blocked", 0)),
                            "mem_fold_events": int(self.counters.get("mem_fold_events", 0)),
                            "mem_fold_removed_tokens_est": int(self.counters.get("mem_fold_removed_tokens_est", 0)),
                            "mem_fold_overflow_tokens_est": int(self.counters.get("mem_fold_overflow_tokens_est", 0)),
                            "mem_unfold_events": int(self.counters.get("mem_unfold_events", 0)),
                            "mem_unfold_added_tokens_est": int(self.counters.get("mem_unfold_added_tokens_est", 0)),
                            "mem_rag_unfold_events": int(self.counters.get("mem_rag_unfold_events", 0)),
                            "prompt_est_total": int(self.counters.get("prompt_est_total", 0)),
                            "prompt_est_active_context_total": int(self.counters.get("prompt_est_active_context_total", 0)),
                            "prompt_est_active_noise_lines": int(self.counters.get("prompt_est_active_noise_lines", 0)),

                        }
                    }
                    self._trace({"type": "finish", "task_id": task_id, "method": method, "run_tag": run_tag, "step": step, "result": {
                        "answer": result["answer"],
                        "explanation": result["explanation"][:2000],
                        "confidence": result["confidence"],
                        "usage": result["usage"],
                        "steps": result["steps"],
                        "elapsed_sec": result["elapsed_sec"],
                        "tool_stats": result["tool_stats"],
                    }})
                    if self.cfg.verbose:
                        print(f"[{run_tag}][{method}][{task_id}] FINISH steps={result['steps']} tok={result['usage'].get('total_tokens')} tools={result['tool_stats']['tool_calls_total']} elapsed={elapsed:.1f}s")
                    return result

                else:
                    self.counters["unknown_tool"] += 1
                    self.mem.record_msg(f"[ASSISTANT] Unknown tool: {tool}")
                    self._trace({"type": "tool", "task_id": task_id, "method": method, "run_tag": run_tag, "step": step, "tool": tool, "args": args, "unknown": True})

                # Drain memory events (fold/unfold decisions) emitted during this step
                self._drain_mem_events(task_id, method, run_tag, step)

            # No finish
            top_rep = self.search_query_counts.most_common(5)
            elapsed = time.time() - start_time

            # Drain any remaining memory events
            self._drain_mem_events(task_id, method, run_tag, step=self.cfg.max_steps)

            result = {
                "answer": "",
                "explanation": "max_steps reached (no finish)",
                "confidence": "0%",
                "active_context": self.mem.get_active_text(),
                "usage": dict(self.usage_accum),
                        "prompt_est_accum": {
                            "total": int(self.counters.get("prompt_est_total", 0)),
                            "system": int(self.counters.get("prompt_est_system", 0)),
                            "user_base": int(self.counters.get("prompt_est_user_base", 0)),
                            "followups": int(self.counters.get("prompt_est_followups", 0)),
                            "active_context_total": int(self.counters.get("prompt_est_active_context_total", 0)),
                            "active_tool_lines": int(self.counters.get("prompt_est_active_tool_lines", 0)),
                            "active_obs_lines": int(self.counters.get("prompt_est_active_obs_lines", 0)),
                            "active_noise_lines": int(self.counters.get("prompt_est_active_noise_lines", 0)),
                            "active_summary_lines": int(self.counters.get("prompt_est_active_summary_lines", 0)),
                            "active_meta_lines": int(self.counters.get("prompt_est_active_meta_lines", 0)),
                            "active_other_lines": int(self.counters.get("prompt_est_active_other_lines", 0)),
                        },
                        "mem_event_stats": {
                            "fold_events": int(self.counters.get("mem_fold_events", 0)),
                            "fold_removed_tokens_est": int(self.counters.get("mem_fold_removed_tokens_est", 0)),
                            "fold_overflow_tokens_est": int(self.counters.get("mem_fold_overflow_tokens_est", 0)),
                            "unfold_events": int(self.counters.get("mem_unfold_events", 0)),
                            "unfold_added_tokens_est": int(self.counters.get("mem_unfold_added_tokens_est", 0)),
                            "rag_unfold_events": int(self.counters.get("mem_rag_unfold_events", 0)),
                            "pef_fold_events": int(self.counters.get("mem_pef_fold_events", 0)),
                            "pef_roll_fold_events": int(self.counters.get("mem_pef_roll_fold_events", 0)),
                        },
                "steps": self.cfg.max_steps,
                "elapsed_sec": elapsed,
                "tool_stats": {
                    "tool_calls_proposed_total": int(self.counters.get("tool_calls_proposed_total", 0)),
                    "tool_calls_total": int(self.counters["tool_calls_total"]),
                    "search_calls": int(self.counters["search_calls"]),
                    "open_page_calls": int(self.counters["open_page_calls"]),
                    "open_page_cache_hits": int(self.counters.get("open_page_cache_hits", 0)),
                    "policy_overrides": int(self.counters.get("policy_overrides", 0)),
                    "constraints_written": int(self.counters.get("constraints_written", 0)),
                    "constraints_write_errors": int(self.counters.get("constraints_write_errors", 0)),
                    "duplicate_open_blocked": int(self.counters.get("duplicate_open_blocked", 0)),
                    "blocked_search_query": int(self.counters.get("blocked_search_query", 0)),
                    "unproductive_searches": int(self.counters.get("unproductive_searches", 0)),
                    "search_queries_cooled_down": int(self.counters.get("search_queries_cooled_down", 0)),
                    "candidate_first_overrides": int(self.counters.get("candidate_first_overrides", 0)),
                    "search_cycle_break_overrides": int(self.counters.get("search_cycle_break_overrides", 0)),
                    "adaptive_unfold_calls": int(self.counters.get("adaptive_unfold_calls", 0)),
                    "adaptive_unfold_activated_nodes": int(self.counters.get("adaptive_unfold_activated_nodes", 0)),
                    "adaptive_unfold_errors": int(self.counters.get("adaptive_unfold_errors", 0)),
                    "mode_switch_overrides": int(self.counters.get("mode_switch_overrides", 0)),
                    "repeated_search_count": int(self.counters["repeated_search_count"]),
                    "unique_search_queries": int(len(self.search_query_counts)),
                    "top_repeated_queries": [{"query": q, "count": c} for q, c in top_rep if c >= 2],
                    "json_parse_failures": int(self.counters["json_parse_failures"]),
                    "json_recoveries": int(self.counters["json_recoveries"]),
                    "premature_finish_blocked": int(self.counters.get("premature_finish_blocked", 0)),
                            "mem_fold_events": int(self.counters.get("mem_fold_events", 0)),
                            "mem_fold_removed_tokens_est": int(self.counters.get("mem_fold_removed_tokens_est", 0)),
                            "mem_fold_overflow_tokens_est": int(self.counters.get("mem_fold_overflow_tokens_est", 0)),
                            "mem_unfold_events": int(self.counters.get("mem_unfold_events", 0)),
                            "mem_unfold_added_tokens_est": int(self.counters.get("mem_unfold_added_tokens_est", 0)),
                            "mem_rag_unfold_events": int(self.counters.get("mem_rag_unfold_events", 0)),
                            "prompt_est_total": int(self.counters.get("prompt_est_total", 0)),
                            "prompt_est_active_context_total": int(self.counters.get("prompt_est_active_context_total", 0)),
                            "prompt_est_active_noise_lines": int(self.counters.get("prompt_est_active_noise_lines", 0)),

                },
            }
            # Attempt best-effort auto-finish using ONLY opened evidence
            if self.cfg.force_finish_on_deadline and self.counters.get("open_page_calls", 0) >= 1:
                ans, expl = self._try_autofinish(user_question)
                if ans:
                    result["answer"] = ans
                    result["explanation"] = expl or result.get("explanation", "")
                    result["confidence"] = "auto"
                    self.counters["forced_finish"] += 1
                    self._trace({"type":"forced_finish","task_id":task_id,"method":method,"run_tag":run_tag,"step":self.cfg.max_steps,"answer":ans})

            self._trace({"type": "finish", "task_id": task_id, "method": method, "run_tag": run_tag, "step": self.cfg.max_steps, "result": result})
            if self.cfg.verbose:
                print(f"[{run_tag}][{method}][{task_id}] NO_FINISH steps={result['steps']} tok={result['usage'].get('total_tokens')} tools={result['tool_stats']['tool_calls_total']} elapsed={elapsed:.1f}s")
            return result
        finally:
            self._close_trace()


    def _try_autofinish(self, user_question: str) -> Tuple[str, str]:
        """Best-effort auto-finish using ONLY opened evidence (no extra tool calls).

        Returns (answer, explanation). If cannot infer, returns ("","...").
        Designed to reduce NO_FINISH when the agent has already opened the required pages.
        """
        q = (user_question or "")
        ql = q.lower()

        def parse_fields(content: str):
            m_year = re.search(r"(?i)start_year\s*:\s*(\d{4})", content or "")
            m_hq = re.search(r"(?i)headquarters\s*:\s*([^\n]+)", content or "")
            m_code = re.search(r"(?i)code_name\s*:\s*([^\n]+)", content or "")
            m_key = re.search(r"(?i)key_number\s*:\s*(\d+)", content or "")
            year = int(m_year.group(1)) if m_year else None
            hq = (m_hq.group(1).strip() if m_hq else None)
            code = (m_code.group(1).strip() if m_code else None)
            key = int(m_key.group(1)) if m_key else None
            return year, hq, code, key

        def build_expl(primary_docid: str) -> str:
            ev = [primary_docid] + [d for d in self.evidence_docids if d != primary_docid]
            ev = [d for d in ev if d][:5]
            return "Evidence docids: " + ", ".join(ev) if ev else "Evidence docids: (none)"

        # Pattern A: earliest start_year + headquarters
        if ("earliest" in ql) and ("start_year" in ql) and ("headquarters" in ql):
            candidates = []

            projects = re.findall(r"Project_\d{4}", q)
            if projects:
                for p in projects:
                    docid_guess = "D_TRUTH_" + p.split("_")[1]
                    content = self.opened_cache.get(docid_guess)
                    did = docid_guess
                    if not content:
                        for odid, cont in self.opened_cache.items():
                            if p in cont:
                                content = cont
                                did = odid
                                break
                    if not content:
                        continue
                    year, hq, _, _ = parse_fields(content)
                    if year is not None and hq:
                        candidates.append((year, p, hq, did))
            else:
                m_pref = re.search(r"(Codename_[A-Za-z0-9]+)", q)
                pref = m_pref.group(1) if m_pref else None
                for did, cont in self.opened_cache.items():
                    year, hq, code, _ = parse_fields(cont)
                    if year is None or not hq:
                        continue
                    if pref and code and code.startswith(pref):
                        proj = "Project_" + did.split("_")[-1] if did.startswith("D_TRUTH_") else "UnknownProject"
                        candidates.append((year, proj, hq, did))

            if candidates:
                candidates.sort(key=lambda x: x[0])
                year, proj, hq, did = candidates[0]
                ans = f"{proj} | {hq}"
                return ans, build_expl(did)

        # Pattern B: two-phase select-by code_name prefix then max key_number, return headquarters
        if (
            ("key_number" in ql)
            and ("code_name" in ql)
            and ("largest" in ql or "maximum" in ql or "max" in ql)
            and ("headquarters" in ql)
            and ("step" in ql or "select" in ql)
        ):
            projects = re.findall(r"Project_\d{4}", q)
            m_init = re.search(r"Codename_([A-Za-z0-9])", q)
            init = (m_init.group(1).lower() if m_init else None)

            if projects and init:
                best = None  # (key, project, hq, docid)
                for p in projects:
                    docid_guess = "D_TRUTH_" + p.split("_")[1]
                    content = self.opened_cache.get(docid_guess)
                    did = docid_guess
                    if not content:
                        for odid, cont in self.opened_cache.items():
                            if p in cont:
                                content = cont
                                did = odid
                                break
                    if not content:
                        continue

                    _, hq, code, key = parse_fields(content)
                    if (hq is None) or (code is None) or (key is None):
                        continue
                    if not code.lower().startswith(f"codename_{init}"):
                        continue
                    cand = (key, p, hq, did)
                    if (best is None) or (cand[0] > best[0]):
                        best = cand

                if best is not None:
                    key, proj, hq, did = best
                    ans = f"{proj} | {hq}"
                    return ans, build_expl(did)

        return "", ""
